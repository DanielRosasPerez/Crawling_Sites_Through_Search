Topic,Title,Description,URL
python,Python Fundamentals,"51+ hours of video instruction.
Overview
The professional programmer’s Deitel® video guide to Python development with the powerful IPython and Jupyter Notebooks platforms.
Description
Python Fundamentals LiveLessons with Paul Deitel is a code-oriented presentation of Python—one of the world’s most popular and fastest growing languages. In the context of scores of real-world code examples ranging from individual snippets to complete scripts, Paul will demonstrate coding with the interactive IPython interpreter and Jupyter Notebooks. You’ll quickly become familiar with the Python language, its popular programming idioms, key Python Standard Library modules and several popular open-source libraries. In the Intro to Data Science videos, Paul lays the groundwork for later lessons in which he’ll introduce some of today’s most compelling, leading-edge computing technologies, including natural language processing, data mining Twitter® for sentiment analysis, cognitive computing with IBM® Watson™, supervised machine learning with classification and regression, unsupervised machine learning with clustering, computer vision through deep learning and convolutional neural networks, sentiment analysis through deep learning with recurrent neural networks, big data with Hadoop®, Spark™ streaming, NoSQL databases and the Internet of Things.
Download the code examples for this LiveLesson from https://github.com/pdeitel/PythonFundamentalsLiveLessons. This repository will be updated with the additional lessons’ examples as the lessons are completed.
About the Instructor
Paul J. Deitel, CEO and Chief Technical Officer of Deitel & Associates, Inc., is a graduate of MIT, where he studied Information Technology. He holds the Sun (now Oracle) Certified Java Programmer and Certified Java Developer certifications, and is an Oracle Java Champion. Through Deitel & Associates, Inc., he has delivered Java, C#, Visual Basic, C++, C and Internet programming courses to industry clients, including Cisco, IBM, Sun Micro systems, Dell, Siemens, Lucent Technologies, Fidelity, NASA at the Kennedy Space Center, the National Severe Storm Laboratory, White Sands Missile Range, Rogue Wave Software, Boeing, SunGard Higher Education, Stratus, Cambridge Technology Partners, One Wave, Hyperion Software, Adra Systems, Entergy, CableData Systems, Nortel Networks, Puma, iRobot, Invensys and many more. He and his co-author, Dr. Harvey M. Deitel, are the world’s best-selling programming-language textbook/professional book authors.
Skill Level
Beginner-to-Intermediate
What you Will Learn in Part I
Before You Begin—Configure your system for Python, obtain the code examples, Python package managers, Paul’s contact info Lesson 1—Test-Drives: Using IPython and Jupyter Notebooks—Work with snippets and scripts in the context of IPython and Jupyter NotebooksLesson 2—Intro to Python Programming—Variables, types, operators, strings, I/O, decisions, objects and dynamic typingLesson 3—Control Statements—if, if…else, if…elif…else, for, while, break, continue, augmented assignments, boolean operators, intro to listsLesson 4—Functions—Custom function definitions, importing libraries, simulation with random-number generation, scope, default parameter values, keyword arguments, arbitrary argument lists, methods, intro to tuples, intro to functional-style programming
What you will learn in Part II:
Lesson 5—Sequences: Lists and Tuples—Create, initialize and access the elements of lists and tuples; sort and search lists, and search tuples; pass lists and tuples to functions and methods; list methods; functional-style programming (lambdas, filter, map, reduce, list comprehensions, generator expressions, 2D lists); static visualization with the Seaborn and Matplotlib visualization libraries.Lesson 6—Dictionaries and Sets—Dictionaries of key—value pairs; sets of unique values; iterating through keys, values and key—value pairs; adding, removing and updating key—value pairs; dictionary and set comparison operators; set operators and methods; operators in and not in for membership testing; mutable set operations; dictionary and set comprehensions; dynamic visualization with the Seaborn and Matplotlib visualization libraries.Lesson 7—Array-Oriented Programming with NumPy—numpy module’s high-performance ndarrays; how ndarrays differ from lists; comparing list vs. ndarrayperformance with the IPython %timeit magic; one-dimensional and multidimensionalndarrays; common ndarray manipulations; introduction to the pandas data manipulation library; one-dimensional Series and two-dimensional DataFrames; custom Series and DataFrame indices; basic descriptive statistics for data in aSeries and a DataFrame; customizing pandas output formatting.
What you will learn in Part III:
Lesson 8—Strings: A Deeper Look—String methods; string formatting; concatenating and repeating strings; stripping whitespace; string comparisons; search strings for substrings and replacing substrings; tokenizing strings; regular expressions for pattern matching, replacing substrings and validating data; manipulating data in pandas.Lesson 9—Files and Exceptions—Text-file processing; serializing objects into the JSON with the json module; with statement for avoiding “resource leaks”; exception handling; try…except statement; else clause; executing code when no exceptions occur in a try suite; finally clause; raise exceptions; more details on tracebacks; stack unwinding; CSV file processing via the csv module; loading and manipulating CSV files in pandas.Lesson 10—Object-Oriented Programming—Custom classes; controlling access to attributes; properties for data access; simulating “private” attributes; Python special methods for customizing string representations; inheritance, duck typing and polymorphism; class object; Python special methods for overloading operators; named tuples; Python 3.7 data classes; unit testing with doctest; namespaces and how they affect scope; Introduction to time series and simple linear regression.
What you will learn in Part IV:
Lesson 11—Natural Language Processing (NLP)—Install and use the TextBlob, NLTK, Textatistic and spaCy NLP libraries;, tokenize text into words and sentences; parts-of-speech tagging (noun, verb, etc.); sentiment analysis (positive, negative or neutral); detect the language of text; translate between languages; get word roots via stemming and lemmatization; spell checking and correction; word definitions, synonyms and antonyms; remove stop words from text; create word-cloud visualizations; determine text readability.Lesson 12—Data Mining Twitter®—Access tweets and other information on Twitter with Tweepy—a popular Python Twitter API client; search past tweets with the Twitter Search API; sample the live tweet stream with the Twitter Streaming API; work with tweet object meta data; use NLP techniques to clean and preprocess tweets for analysis; perform sentiment analysis on tweets; spot trending topics with Twitter’s Trends API; map tweets using folium and OpenStreetMap.Lesson 13—IBM Watson® and Cognitive Computing—Intro to Watson and its free Lite tier services; demos of several Watson services; registering for an IBM Cloud account; set up and get credentials for Watson services; install the Watson Developer Cloud Python SDK; build a Traveler’s companion language translator app that mashes up the Watson Speech to Text, Language Translator and Text to Speech services.
What you will learn in Part V’s case studies:
Lesson 14—Machine Learning: Classification, Regression and Clustering—Use scikit-learn with popular datasets to perform machine learning studies; Use Seaborn and Matplotlib to visualize and explore data; Perform supervised machine learning with k-nearest neighbors classification and linear regression; Perform multi-classification with Digits dataset; Divide a dataset into training, testing and validation sets; Tune hyperparameters with k-fold cross-validation; Measure model performance; Display a confusion matrix showing classification prediction hits and misses; Perform multiple linear regression with the California Housing dataset; Perform dimensionality reduction with PCA and t-SNE on the Iris and Digits datasets to prepare them for two-dimensional visualizations. Perform unsupervised machine learning with k-means clustering and the Iris dataset.Lesson 15—Deep Learning—What a neural network is and how it enables deep learning; Create Keras neural networks;Keras layers, activation functions, loss functions and optimizers; Use a Keras convolutional neural network (CNN) trained on the MNIST dataset to build a computer vision application that recognizes handwritten digits; Use a Keras recurrent neural network (RNN) trained on the IMDb dataset to create a sentiment analysis application that performs binary classification of positive and negative movie reviews.Lesson 16—Big Data: Hadoop, Spark, 17—Manipulate a SQLite relational database using SQL; Understand the four major types of NoSQL databases; Store tweets in a MongoDB NoSQL JSON document database and visualize them on a Folium map; Apache Hadoop and how it’s used in big-data batch-processing applications; Build a Hadoop MapReduce application on Microsoft’s Azure HDInsight cloud service; Apache Spark and how it’s used in high-performance, real-time big-data applications; Process mini-batches of data with Spark streaming; Internet of Things (IoT) and the publish/subscribe model; Publish messages from a simulated Internet-connected device and visualize messages in a dashboard; Subscribe to PubNub’s sample live streams and visualize the data.
LiveLessons Video Training series publishes hundreds of hands-on, expert-led video tutorials covering a wide selection of technology topics designed to teach you the skills you need to succeed. This professional and personal technology video series features world-leading author instructors published by your trusted technology brands: Addison-Wesley, Cisco Press, IBM Press, Pearson IT Certification, Prentice Hall, Sams, and Que. Topics include: IT Certification, Programming, Web Development, Mobile Development, Home & Office Technologies, Business & Management, and more. View All LiveLessons on InformIT: http://www.informit.com/imprint/series_detail.aspx?ser=2185116",/videos/python-fundamentals/9780135917411/
python,"Python Crash Course, 2nd Edition","This is the second edition of the best selling Python book in the world. Python Crash Course, 2nd Edition is a straightforward introduction to the core of Python programming. Author Eric Matthes dispenses with the sort of tedious, unnecessary information that can get in the way of learning how to program, choosing instead to provide a foundation in general programming concepts, Python fundamentals, and problem solving. Three real-world projects in the second part of the book allow readers to apply their knowledge in useful ways.
Readers will learn how to create a simple video game, use data visualization techniques to make graphs and charts, and build and deploy an interactive web application. Python Crash Course, 2nd Edition teaches beginners the essentials of Python quickly so that they can build practical programs and develop powerful programming techniques.
Uses Python 3",/library/view/python-crash-course/9781492071266/
python,"Fluent Python, 2nd Edition","Python’s simplicity lets you become productive quickly, but often this means you aren’t using everything it has to offer. With the updated edition of this hands-on guide, you’ll learn how to write effective, modern Python 3 code by leveraging its best ideas.
Don’t waste time bending Python to fit patterns you learned in other languages. Discover and apply idiomatic Python 3 features beyond your past experience. Author Luciano Ramalho guides you through Python’s core language features and libraries and teaches you how to make your code shorter, faster, and more readable.
Featuring major updates throughout the book, Fluent Python, second edition, covers:
Special methods: The key to the consistent behavior of Python objectsData structures: Sequences, dicts, sets, Unicode, and data classesFunctions as objects: First-class functions, related design patterns, and type hints in function declarationsObject-oriented idioms: Composition, inheritance, mixins, interfaces, operator overloading, static typing and protocolsControl flow: Context managers, generators, coroutines, async/await, and thread/process poolsMetaprogramming: Properties, attribute descriptors, class decorators, and new class metaprogramming hooks that are simpler than metaclasses",/library/view/fluent-python-2nd/9781492056348/
python,"Python for Data Analysis, 2nd Edition","Get complete instructions for manipulating, processing, cleaning, and crunching datasets in Python. Updated for Python 3.6, the second edition of this hands-on guide is packed with practical case studies that show you how to solve a broad set of data analysis problems effectively. You’ll learn the latest versions of pandas, NumPy, IPython, and Jupyter in the process.
Written by Wes McKinney, the creator of the Python pandas project, this book is a practical, modern introduction to data science tools in Python. It’s ideal for analysts new to Python and for Python programmers new to data science and scientific computing. Data files and related material are available on GitHub.
Use the IPython shell and Jupyter notebook for exploratory computingLearn basic and advanced features in NumPy (Numerical Python)Get started with data analysis tools in the pandas libraryUse flexible tools to load, clean, transform, merge, and reshape dataCreate informative visualizations with matplotlibApply the pandas groupby facility to slice, dice, and summarize datasetsAnalyze and manipulate regular and irregular time series dataLearn how to solve real-world data analysis problems with thorough, detailed examples",/library/view/python-for-data/9781491957653/
python,"Introducing Python, 2nd Edition","Easy to understand and fun to read, this updated edition of Introducing Python is ideal for beginning programmers as well as those new to the language. Author Bill Lubanovic takes you from the basics to more involved and varied topics, mixing tutorials with cookbook-style code recipes to explain concepts in Python 3. End-of-chapter exercises help you practice what you’ve learned.
You’ll gain a strong foundation in the language, including best practices for testing, debugging, code reuse, and other development tips. This book also shows you how to use Python for applications in business, science, and the arts, using various Python tools and open source packages.",/library/view/introducing-python-2nd/9781492051374/
python,Leveraging the disruptive power of artificial intelligence for fairer opportunities,"According to President Obama’s Council of Economic Advisers (CEA), approximately 3.1 million jobs will be rendered obsolete or permanently altered as a consequence of artificial intelligence technologies. Artificial intelligence (AI) will, for the foreseeable future, have a significant disruptive impact on jobs. That said, this disruption can create new opportunities if policymakers choose to harness them—including some with the potential to help address long-standing social inequities. Investing in quality training programs that deliver premium skills, such as computational analysis and cognitive thinking, provides a real opportunity to leverage AI’s disruptive power.Makada Henry-NickieFellow - Governance Studies  TwittermhnickieAI’s disruption presents a clear challenge: competition to traditional skilled workers arising from the cross-relevance of data scientists and code engineers, who can adapt quickly to new contexts. Data analytics has become an indispensable feature of successful companies across all industries. This reality dictates that companies invest heavily in data analytics to remain competitive and profitable. Consequently, unlikely industries such as retail, banking, finance, and even agricultural firms are aggressively competing for talent with specific computational data science and programming skills. A recent IBM report expertly quantifies the scope and breadth of employers’ hiring demands, noting that “[d]emand for data-driven decision makers, such as data-enabled marketing managers, will comprise one-third of the data savvy professional job market, with a projected increase of 110,000 positions by 2020.” Herein lies a window of opportunity: the rapidly growing technical skills gap.Investing in high-quality education and training programs is one way that policymakers proactively attempt to address the workforce challenges presented by artificial intelligence. It is essential that we make affirmative, inclusive choices to ensure that marginalized communities participate equitably in these opportunities.Policymakers should prioritize understanding the demographics of those most likely to lose jobs in the short-run. As opposed to obsessively assembling case studies, we need to proactively identify policy entrepreneurs who can conceive of training policies that equip workers with technical skills of “long-game” relevance. As IBM points out, “[d]ata democratization impacts every career path, so academia must strive to make data literacy an option, if not a requirement, for every student in any field of study.”Machines are an equal opportunity displacer, blind to color and socioeconomic status.Machines are an equal opportunity displacer, blind to color and socioeconomic status. Effective policy responses require collaborative data collection and coordination among key stakeholders—policymakers, employers, and educational institutions—to  identify at-risk worker groups and to inform workforce development strategies. Machine substitution is purely an efficiency game in which workers overwhelmingly lose. Nevertheless, we can blunt these effects by identifying critical leverage points.Investing in innovative education and training is an excellent place to start. Bill Gates’ recent $1.7 billion investment in U.S. public schools is a sign of the way forward, which offers two compelling messages for policymakers. First, innovate and experiment until we identify the right policies. Second, prioritize high-needs schools in poor neighborhoods; they deserve distinct attention to close their opportunity gaps and prepare them to be competitive in the future workforce.Policymakers can choose to harness AI’s disruptive power to address workforce challenges and redesign fair access to opportunity simultaneously. We should train our collective energies on identifying practical policies that update our current agrarian-based education model, which unfairly disadvantages children from economically segregated neighborhoods. Evidence from a Harvard and New York University research study suggests attending a high-quality high school increases a student’s chances of attending a four-year college; which by extension improves their future income earning potential.Let me ask a bold question: how much do we lose if we experiment with substituting an entry-level data science class for machine shop or a vocational carpentry program in urban high schools and community colleges? A 2010 pilot partnership between the University of California, Los Angeles and the National Science Foundation is an encouraging sign; the pilot focuses on redesigning computer science curricula in urban high schools to include newer mobile technologies and computational analysis.Related Technology & InnovationHow artificial intelligence is transforming the worldDarrell M. West and John R. AllenTuesday, April 24, 2018 Technology & InnovationTrends in the Information Technology sectorMakada Henry-Nickie, Kwadwo Frimpong, and Hao SunFriday, March 29, 2019 Technology & InnovationAlgorithmic bias detection and mitigation: Best practices and policies to reduce consumer harmsNicol Turner Lee, Paul Resnick, and Genie BartonWednesday, May 22, 2019Data science is an applied computational technology best suited to inquisitive minds, making it appropriate for young students. Google’s TensorFlow is an open source machine-learning platform; its free price tag makes the platform an accessible and scalable training resource for schools with constrained budgets. Introducing a data science program into urban schools would be a major paradigm shift for these students. An applied data science program teaching gateway coding skills such as Python, R, SQL, and computational analysis would boost employment possibilities and create meaningful pathways to economic mobility.I am suggesting that we leverage AI’s transformative power to disrupt diminishing possibilities for marginalized groups, like young men of color, who often do not feature in innovative-themed discussions outside of the social justice arena. Open Source groups such as Code.org and StudentRND exemplify the kinds of transformational approaches that democratize access and opportunity.Producing a diverse pipeline of tech-savvy workers for Google and Amazon, even if only at the entry level, is a more attainable dream for most cities than competing in a stacked race for Amazon’s HQ2. Broadening adoption of artificial intelligence technologies poses significant workforce challenges, but it also offers the chance to blunt these effects and create opportunities for marginalized groups if we act preemptively.Google is a donor to the Brookings Institution. The findings, interpretations, and conclusions posted in this piece are solely those of the authors and not influenced by any donation.",https://www.brookings.edu/blog/techtank/2017/11/16/leveraging-the-disruptive-power-of-artificial-intelligence-for-fairer-opportunities/
python,The Hutchins Center Explains: Budgeting for aging America,"For decades, we have been hearing that the baby-boom generation was like a pig moving through a python–bigger than the generations before and after. That’s true. But that’s also a very misleading metaphor for understanding the demographic forces that are driving up federal spending: They aren’t temporary. The generation born between 1946 and 1964 is the beginning of a demographic transition that will persist for decades after the baby boomers die, the consequence of lengthening lifespans and declining fertility. Putting the federal budget on a sustainable course requires long-lasting fixes, not short-lived tweaks.  First, a few demographic facts.As the chart below illustrates, there was a surge in births in the U.S. at the end of World War II, a subsequent decline, and then an uptick as baby boomers began having children.Although the population has been rising, the number of births in the U.S. the past few years has been below the peak baby-boom levels, possibly because many couples chose not to have children during bad economic times. More significant, fertility rates–roughly the number of babies born per woman during her lifetime–have fallen well below pre-baby-boom levels.Meanwhile, Americans are living longer. In 1950, a man who made it to age 65 could expect to live until 78 and a woman until 81. Social Security’s actuaries project that a man who lived to age 65 in 2010 will reach 84 and a woman age 86.Put all this together, and it’s clear that a growing fraction of the U.S. population will be 65 or older.   The combination of longer life spans and lower fertility rates means the ratio of elderly (over 65) to working-age population (ages 20 to 64) is rising. As the chart below illustrates, the ratio will rise steadily as more baby boomers reach retirement age–and then it levels off.  Related Books Autonomous VehiclesBy Clifford Winston and Quentin Karpilow 2020 Making College WorkBy Harry J. Holzer and Sandy Baum 2017 Dream HoardersBy Richard V. Reeves 2017Simply put, this doesn’t look like a pig in a python.  So what do these demographic facts portend for the federal budget?  In simple dollars and cents, the federal government spends more on the old than the young. More older Americans means more federal spending on Social Security and Medicare, the health insurance program for the elderly. On top of that, health care spending per person is likely to continue to grow faster than the overall economy.The net result: 85 percent of the increase in federal spending that the Congressional Budget Office projects for the next 10 years, based on current policies, will go toward Social Security, Medicare and other major federal health programs, and interest on the national debt. David WesselDirector - The Hutchins Center on Fiscal and Monetary Policy Senior Fellow - Economic Studies  TwitterdavidmwesselLouise SheinerThe Robert S. Kerr Senior Fellow - Economic Studies Policy Director - The Hutchins Center on Fiscal and Monetary Policy Restraining future deficits and the size of the federal debt mean restraining spending on these programs or raising taxes–and probably both. One-time savings or minor tweaks won’t suffice. Nor will limiting the belt-tightening to annually appropriated spending.The fundamental fiscal problem is not coping with the retirement of the baby boomers and then going back to budgets that resemble those of the past. The fundamental fiscal problem is that retirement of the baby boomers marks a major demographic transition for the nation, one that will require long-lived changes to benefit programs and taxes.Editor’s Note: This post originally appeared on The Wall Street Journal’s Washington Wire on December 18, 2015.",https://www.brookings.edu/blog/up-front/2015/12/21/the-hutchins-center-explains-budgeting-for-aging-america/
python,The Silicon Valley Wage Premium,"Software application developers earn large salaries in the United States, $96,260 a year on average. But in metropolitan San Jose they earn $131,270, the highest in the country. There are many partial explanations for this—local cost of living, differences in education levels, experience, and industry—but none of them quite account for it. It turns out that developers living in San Jose have acquired the specific skills most valued by employers. As the map below shows, there is a huge amount of variation in earnings for software application developers across regional labor markets. In large metropolitan areas like New York, they earn $105,000, but in Louisville, they earn just $72,000. Average Salary of Software Application Developers by Metropolitan Area, 2013  Similar patterns could be shown for other occupations, of course; for even within the same job title, people vary by education and experience, and regions vary by company and industry mix, productivity and export orientation, which all affect salaries and regional housing prices. The surprising thing, when it comes to software developers and other skilled occupations too, is that none of these factors can fully account for the San Jose premium. Software developers in San Jose are typically slightly less experienced, and while their levels of education are higher—including their likelihood of having majored in engineering or computer science—the difference is not enough to explain their elevated earnings. Likewise, the cost of living in San Jose is remarkably high, but comparable to other major cities.  Related Books Global CitiesBy Greg Clark 2016 The Metropolitan RevolutionBy Bruce Katz and Jennifer Bradley 2014 GreeceBy Theodore Pelagidis and Michael Mitsopoulos 2014So what distinguishes San Jose software developers?  To figure this out, I analyzed a database of 29 million job vacancies advertised online during 2013 as compiled by the analytics firm Burning Glass. Of these, roughly 1.4 million were for software application developers, making it the most in-demand occupation. In total, 3 million ads also contained salary information, which I could use to estimate the average value of each distinct skill advertised.  Jonathan RothwellNonresident Senior Fellow - Metropolitan Policy Program  Twitterjtrothwell In San Jose, the skills advertised for software developers are particularly valuable. The average vacancy requires higher value skills in San Jose than almost any other metropolitan area, even using national rather than local salary values.   For example, 8.4 percent of ads for software developers in San Jose requested Java, a widely used programming language, associated with an average salary of $98,000 across all U.S. ads mentioning both it and a salary requirement. Yet, for the United States as a whole, just 5.7 percent of software developer ads required Java. In New York City, the share was 6.7, and it was 4.7 in Louisville. Other high-value programming languages and skills were disproportionately advertised in San Jose, such as Linux, C++, Python and the term “software engineering.”  These skills were much less commonly required for jobs in Louisville and even New York. Only 0.2 percent of software jobs required Python in Louisville and 1.7 in New York City, compared to 2.8 percent in San Jose. It is valued at $100,345. These and other skills contribute to the high premium enjoyed by Silicon Valley computer workers, but they could be profitably learned by a much larger swath of people, as online educators like Treehouse, Udacity, and Code Fellows aim to demonstrate.",https://www.brookings.edu/blog/the-avenue/2014/08/06/the-silicon-valley-wage-premium/
python,Idea to Retire: Old methods of policy education,"Public policy and public affairs schools aim to train competent creators and implementers of government policy. While drawing on the principles that gird our economic and political systems to provide a well-rounded education, like law schools and business schools, policy schools provide professional training. They are quite distinct from graduate programs in political science or economics which aim to train the next generation of academics. As professional training programs, they add value by imparting both the skills which are relevant to current employers, and skills which we know will be relevant as organizations and societies evolve. The relevance of the skills that policy programs impart to address problems of today and tomorrow bears further discussion. We are living through an era in which societies are increasingly interconnected. The wide-scale adoption of devices such as the smartphone is having a profound impact on our culture, communities, and economy. The use of social and digital media and associated means of communication enabled by mobile devices is changing the tone, content, and geographic scope of our conversations, modifying how information is generated and consumed, and changing the very nature of citizen engagement. Information technology-based platforms provisioned by private providers such as Facebook, Google, Uber, and Lyft maintain information about millions of citizens and enable services such as transportation that were mediated in the past solely by the public sector. Surveillance for purposes of public safety via large-scale deployment of sensors also raises fundamental questions about information privacy. From technology-enabled global delivery of work to displacement and replacement of categories of work, some studies estimate that up to 47 percent of U.S. employment might be at risk of computerization with an attendant rise in income inequality. These technology-induced changes will affect every policy domain. How should policy programs best prepare students to address societal challenges in this world that is being transformed by technology? We believe the answer lies in educating students to be “men and women of intelligent action.” A model of policy educationWe begin with a skills-based model of policy education. These four essential skills address the general problems policy practitioners frequently face:Design skills to craft policy ideas Analytical skills to make smart ex ante decisions Interpersonal experience to manage policy implementation  Evaluative skills to assess outcomes ex post and correct course if necessaryThese skills make up the policy analysis toolkit required to be data driven practitioner of “intelligent action” in any policy domain. This toolkit needs to be supplemented by an understanding of how technology is transforming societal challenges, enabling new solutions, or disrupting existing regulatory regimes. This understanding is essential to policy formulation and implementation. Pillar 1: Design skillsAs with engineering, where design precedes analysis, this first pillar seeks to educate students in thinking creatively about problems in order to devise and develop policy ideas. Using ideas derived from design, divergent and convergent thinking principles are employed to generate, explore, and arrive at a candidate set of solutions. Using Uber as an example, an approach to identify and explore the key policy issues such as convenience, costs, driver working hours, and insurance would involve interviewing and observing both incumbent taxi drivers and Uber drivers. This in turn would lead to a set of alternatives that deserve further and careful consideration.  Using these skills, candidate designs and choices that are generated can be evaluated using the policy analytic toolkit. Pillar 2: Analytical skillsAt Carnegie Mellon, we are often cited in media and interrogated by peers on our approach to analytical and technology skills education. Curiosity about which skills are the “right” skills to teach policy practitioners are common, but we believe this is the wrong approach. We instead begin from the premise that policy or management decisions should be grounded in evidence.  We then determine the skills required to assemble the types of evidence that will likely be available to policy makers in the future.  In increasingly instrumented environments where citizens and infrastructure produce continuous streams of data, making sense of it all will require a somewhat different set of skills. We believe that a grounding in micro-economics, operations research, statistics, and program evaluation (aka causal inference) to be an essential core to policy programs. New coursework will teach students to work with multi-variable data and machine learning with an emphasis on prediction. This material ought to be part of the required coursework in statistics given the importance of prediction in many policy implementation settings. Along the same lines, the ability to work with unstructured data (especially text) and data visualization will become increasingly relevant to all students, not just those students who want to specialize in data analytics. Finally, knowledge of data manipulation and analysis languages such as Python and R for analytic work will be important because data often has to be massaged and cleansed prior to analysis. An important task for programs will be to determine the competencies expected of graduates. Pillar 3: Interpersonal experiencesThe third pillar of the skills-based model is interpersonal experience, where the practiced habits of good communication and steady negotiation developed with a sound understanding of organizations, their design and their behaviors. We label these purposely as experiences rather than skills because we believe they are best practiced either in the real-world or in simulated real-world settings. It is also in this pillar where practitioners learn the knowledge necessary to become credible experts in their domain. We believe that in addition to core coursework in the area, a supplementary curriculum which provides students with opportunities to gain these experiences is an essential component of our educational model.Pillar 4: Evaluative skillsRelated Books Turning PointBy Darrell M. West and John R. Allen 2020 MarijuanaBy John Hudak 2020 Divided Politics, Divided NationBy Darrell M. West 2020The ability to carefully diagnose the effectiveness of policy or management interventions is the fourth pillar of our model. It is insufficient to create and execute policy without measurement, and this is where both careful thought to the fundamental issues of measurement and evaluation become important. The ability to make objective judgments on the benefits, liabilities, and unintended consequences of prior policies is the goal of this set of skills. Here, sound statistical and econometric training with an understanding of the principles of causal inference is essential. In addition, program evaluation skills such as cost-benefit and financial analysis help practitioners round out their evaluation skills by considering both non-monetary and economic impacts.What should be retired?A skills-based approach might replace certain aspects of existing policy training.  This depends on a number of factors specific to each institution, but three generally applicable observations are clear. First, real-world experiences are a powerful way to encode domain learning as well as project management skills. Through project-based work, students can learn about institutional contexts in specific policy domains and political processes such as budgeting. Second, team-based projects allow students to learn and apply principles of management and organizational behavior. At Carnegie Mellon, we refer to these as “systems synthesis” projects, since they require students to adopt a systemic point of view and to synthesize a number of skills in their policy analysis toolkit. Third, interpersonal skills training can be practiced through activities such as weekend negotiation exercises, hackathons, and speaker series. These activities can be highly intentional and fashioned to reinforce skills rather than as a recess from the “real work” of classroom training. Since students complete graduate programs in such a short time, counseling them to focus on outcomes from day one will allow them to choose a reinforcing set of coursework and real-world experiences. In summary, we argue for a model of policy education that views practitioners as future problem solvers. Good policy education must consider the ways in which problems will present themselves, and the ways in which answers will obscure themselves. Rigorous training grounded in the analysis of available evidence and buoyed by real-world interpersonal experiences is a sound approach to relevant, durable policy training. RRamayya KrishnanRamayya Krishnan is the dean of H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University where he is the W.W. Cooper and Ruth F. Cooper Professor of Management Science and Information Systems.JJon NehlsenJon Nehlsen is senior director of external relations at H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University.Read other essays in the Ideas to Retire blog series here.",https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/
python,"Skills, success, and why your choice of college matters","Amidst growing frustration with the cost of higher education, complaints also abound about its quality. One critique, launched in the book Academically Adrift by two sociologists, finds little evidence that college students score better on measures of critical thinking, writing, and reasoning after attending college. This is something of a paradox, since strong evidence shows that attending college tends to raise earnings power, even for students who start with mediocre preparation. Our recent study uses a different approach to assess the value of a college education. We find that the particular skills listed by a college’s alumni on their resumes predict how well graduates from those schools perform in terms of earning a living, meeting debt obligations, and working for high-paying or innovative companies. Since jobs requiring more valuable skills typically require at least some college education, this finding suggests many students are gaining valuable skills from college. But the variation in alumni skills across schools is wide, even after considering the aptitude of the students in terms of their pre-admission test scores. This variation implies that what one studies and where have big effects on economic outcomes.Skills versus degreesIt is widely known that education raises individual earnings, but education—measured in years of study or level of degree—is a very rough measure of learning. Thus, it is not surprising that studies consistently find that skills are an important predictor of economic outcomes. People with higher test scores—another measure of learning—earn higher wages, even with the same level of education. Likewise, graduates with technical degrees earn more, as do workers in occupations requiring more STEM skills. At the international scale, performance on standardized exams has a much stronger statistical relationship with economic outcomes than do years of education, according to a new OECD study.How we valued skills by collegeUsing data from the company Burning Glass, we calculated the average salary listed for distinct skills based on 3 million job vacancy ads. To match these skills to colleges, we used data from LinkedIn’s college profile pages, which show the 25 most common skills (e.g., customer service, Microsoft Excel, Python) listed by alumni from each college. For the average college, we observed 1,150 profiles per skill. (A great advantage of using LinkedIn data is the large sample size.) We obtained data for 2,164 colleges representing profiles for 2.5 million U.S. residents who attended college. By comparison, Academically Adrift surveyed 2,300 college graduates.Alumni with more valuable skills earn higher salariesMeasured at mid-career (meaning at least 10 years of working), salaries tend to be much higher for alumni who list high-value skills on their resumes. Earnings go up by an average of $2,600 for every decile of skill. Our more detailed empirical work shows that skills predict higher earnings even after controlling for math test scores on the ACT and SAT, as well as other student characteristics like family income.Cal Tech graduates list the highest-value skills (e.g., Matlab, Python, C++, algorithms, and machine learning) and typically earn $126,000 at mid-career. Other four-year schools with high-value skills and high salaries include Harvey Mudd, MIT, the Polytechnic Institute of New York University, and the Air Force Academy. Earnings data from two-year colleges are not as widely available, and the correlation with alumni skills is weaker, but alumni from those schools also seem to benefit from higher skills training. Top schools include the Pittsburg Institute of Aeronautics, Spartan College of Aeronautics and Technology (Tulsa, Okla.), Coleman University (San Diego), Hondros College (Columbus, Ohio), and SUNY College of Technology at Alfred.Alumni with more valuable skills have higher loan repayment ratesRelated Books Global CitiesBy Greg Clark 2016 The Metropolitan RevolutionBy Bruce Katz and Jennifer Bradley 2014 GreeceBy Theodore Pelagidis and Michael Mitsopoulos 2014As an alternative to mid-career earnings, we also analyzed how skills predict the ability to make student loan payments immediately after graduation. Here too, more valuable skills translate into labor market success. For example, not a single Harvey Mudd attendee between 2009 and 2011 defaulted on his or her federal loans within three years of leaving. Repayment rates average 95 percent for four-year colleges in the top 10 percent for alumni skills but 87 percent for those in the bottom 10 percent. For two-year colleges, repayment rates are uniformly lower, but colleges offering higher-value skills still have significantly higher repayment rates than those that do not.Alumni with more valuable skills are more likely to work for top organizationsAnother outcome measure is whether alumni work for a desirable company or organization. LinkedIn lists the 25 enterprises that employ the most alumni from each school. To quantify the value of working for a given entity, employers were coded for desirability with data from a 2014 survey of 46,000 U.S. college students in 329 universities, developed by Universum, a corporate marketing intelligence company. A total of 212 employers, including government agencies, made it onto a top 100 list for at least one group of student majors. The most desirable employers across majors were Google, Disney, Apple, Microsoft, the FBI, Nike, NASA, the Environmental Protection Agency, the Peace Corps, and Facebook. For the top 10 percent of four-year colleges on alumni skills, half of LinkedIn alumni profiles indicate employment at one of the 212 top-rated companies, compared to just one in four for schools in the bottom 10 percent. For two-year schools, nearly two in five alumni (37 percent) of top-tier schools by skill worked for a top company, versus one in five alumni (21 percent) of bottom-tier schools.For placement at Google specifically, Harvey Mudd has the highest rate—2 percent of all alumni—followed by Stanford, Carnegie Mellon, Caltech, and MIT. Almost all of the colleges with the highest placement rates at Google are in the top 20 percent of alumni skills, including liberal arts colleges like Swarthmore, Pomona, Claremont, McKenna, and Williams. Jonathan RothwellNonresident Senior Fellow - Metropolitan Policy Program  TwitterjtrothwellAlumni with more valuable skills are more likely to work for innovative organizationsWorkers who contribute to the creation and development of new, valuable products can lift the living standards of people around the world. Companies that patent are more likely to be creating these sorts of advanced industry products, and 843 entities, including universities and government agencies, own at least 40 patents granted by the U.S. Patent and Trademark Office in 2014. Four-colleges that graduate alumni in the top 10 percent by skill are twice as likely to have graduates working at a top patenting organization than are colleges in the bottom 10 percent (3.3 versus 1.6 percent). Likewise, graduates from two-year colleges are about twice as likely to be working for a patenting entity if their school is in the top 10 percent compared to the bottom (1.9 versus 0.9 percent).Schools with high placement rates at patenting entities include those listed above, as well as less the U.S. Naval Academy, Lawrence Technological University, the Stevens Institute of Technology, Santa Clara University, Brazosport College, Mount Mercy University, University of Texas-Dallas, the Missouri University of Science and Technology, and San Jose State University.How to judge collegesEarnings and other economic outcomes should not be equated with social value, and there are plenty of jobs and professions—child care, teaching, social work—that pay modestly but are nevertheless highly valuable to society. Colleges that specialize in this training or instill even moderately valuable skills in the least academically prepared students may be socially important institutions even if their alumni frequently are less affluent.Nonetheless, earnings clearly matter privately and socially, as does work that supports innovation and highly productive advanced industries. Many colleges offer programs of study in fields that appear to have almost no market value—nor even any social value since the knowledge acquired is never put to use, at least through paid employment. In this sense, how well colleges instill highly valuable skills and prepare students to contribute productively to the economy should be an important consideration when evaluating schools. Colleges that do this for the students least likely to otherwise succeed are offering an even more beneficial service, as we have discussed in our value-added college research.Correction: A previous version of this post showed graphs which reversed the label on 2- and 4-year colleges. The graphs have been corrected.",https://www.brookings.edu/opinions/skills-success-and-why-your-choice-of-college-matters/
python,An Atlanta organization’s mission to bring racial equity to the tech ecosystem,"SummaryBetween the COVID-19 pandemic and the tragic death of George Floyd, the country’s ongoing crisis of racism has come into stark relief. Black Americans are disproportionately diagnosed with or dying from COVID-19 due to structural conditions, while also facing major economic risks as the racial unemployment gap between white and Black populations is the widest it’s been in five years. At the same time, Black people are still vulnerable to police violence that too often occurs without consequences. While there is a great deal of work to be done to dismantle structural racism, it is imperative to use this moment to remove racial barriers and invest in long-term prosperity for Black people, enterprises, and communities.Reniya DinkinsSenior Research Assistant - Metropolitan Policy Program  TwitterreniyasdinkinsSifan LiuSenior Research Analyst - Metropolitan Policy Program  TwitterSifan_LiuRodney SampsonNonresident Senior Fellow - Metropolitan Policy Program  TwitterRodneySampsonJJustin SampsonChief of Staff - OHUB Closing the racial wealth divide can create better health, educational, and economic outcomes for Black Americans. The tech industry, which has both a significant need for more racial equity and a plethora of jobs that don’t require an expensive college degree, can offer pathways to higher incomes and opportunities for wealth creation. Rodney Sampson recognized this when he co-founded Opportunity Hub (OHUB), a technology, startup, and venture ecosystem-building platform created to ensure that everyone, everywhere has equitable access to the future of work and the fourth industrial revolution as a path to multigenerational wealth creation with no reliance on preexisting wealth. Leveraging its metrics-driven blueprint and methodology—as outlined in “Building Inclusive Entrepreneurship Ecosystems In Communities of Color” with the Federal Reserve Bank of Kansas City—OHUB delivers early exposure, skills development, talent placement, entrepreneurship-support programming, and market and capital access. This is driven by racial equity as well as diversity, equity, and inclusion solutions; innovation labs and equity districts; college campus programs; skills and training programs; talent placement efforts; entrepreneurial support and startup acceleration; and capital formation.Execution In 2013, Rodney Sampson and his wife, Shanterria Sampson, co-founded Opportunity Hub after the release of Sampson’s fourth book, Kingonomics: Twelve Innovative Currencies for Transforming Your Business and Life Inspired by Dr. Martin Luther King, Jr. The book asserts that equitable access to the innovation economy creates systemic change, economic justice, and an accelerated path out of poverty and income inequality.After hearing the needs of thousands of aspiring Black tech startup founders at the Kingonomics’ large-scale innovation, entrepreneurship, and investment conferences hosted by Sampson and ABC’s Shark Tank, OHUB launched in Atlanta and quickly became the largest Black-owned multicampus coworking entrepreneurship center and technology hub in the United States. It is definitively focused on diversity, inclusion, and equity as a business and investment thesis for the development of high-demand technical talent, net new permanent jobs, and high-growth company-building. Each year, thousands of people learn, engage, build, and work via the OHUB ecosystem.OHUB’s blueprint for building ground-up, hyperlocal, globally connected, diverse, equitable, and inclusive technology, startup, and venture ecosystems is represented by the Economic Development Pyramid below and executed across its wholly owned subsidiaries and sister organizations. OHUB executes these economic development efforts with diversity, equity, and inclusion solutions, innovation labs and equity districts, college campus programs, skills and training programs, talent placement efforts, entrepreneurial support and startup acceleration, and capital formation.Racial equity and DEIS (diversity, equity, and inclusion solutions)Creating racial equity throughout the technology, startup, and venture ecosystem to create shared prosperity, economic mobility, and new multigenerational wealth for everyone is foundational and core to OHUB’s definitive purpose and mission. To execute, OHUB partners with leading organizations, influencers, platforms, and enterprises to aggregate industry- or ecosystem-wide racial equity pledges to implement DEIS value-added services, strategy, and solutions across an organization’s corporate governance, human resources, procurement, product development (corporate innovation), go-to-market, impact, and investment. Existing partners include the Brookings Institution, VentureBeat, and Brad Feld. Annual fees include the fair market value of a full-time chief diversity, equity, and inclusion officer’s salary and the costs associated with any initiatives created as a result of the DEIS process.OHUB’s West Midtown Campus (circa 2014-2016)Innovation labs and equity districts (safe spaces)OHUB opened its first 7,500-square-foot campus in downtown Atlanta in the summer of 2013. In September 2014, they added 14,000 square feet, with an expansion to West Midtown’s Giant Lofts and the historic Westside in a joint venture with the late Herman J. Russell. In late 2015, OHUB joined forces with local entrepreneurs and venture capitalists to open a 25,000-square-foot campus in Georgia Tech’s Technology Square. This partnership has invested in over 40 high-growth tech startups that have raised more than $400 million in follow on capital, are valued at $1.5 billion, and employ more than 2,000 people. Today, OHUB operates an ecosystem-building lab in the historic 18th & Vine District in Kansas City, Mo. and an innovation lab with the Morehouse Innovation & Entrepreneurship Center in Atlanta.In June 2020, OHUB formed OHUB Development as a wholly owned subsidiary to joint venture with real estate developers via public-private partnerships to build “equity districts” as mixed-use residential, commercial, and retail community concepts that will house OHUB’s early exposure, immersive learning, startup support, and investing programs in socially disadvantaged communities and/or Opportunity Zones in key U.S. cities. OHUB joins as a co-developer and investor, also leveraging a small monthly fee per square foot to fund the programs and innovation spaces and labs.HBCU@SXSW Day at Huston-Tillotson University.College campus programsOHUB has a growing community of over 5,000 college students representing over 400 colleges and universities, including our nation’s historically Black colleges and universities. Membership includes access to OHUB.365 (detailed below), priority application to leading festivals and conferences such as SXSW, HIMSS, BlackInAI (NEURIPS), VentureBeat’s Transform AI, and LA Blockchain Summit. Tech companies sponsor student memberships, scholarships and the conferences to enable the programming and interview the students for paid summer and full-time roles upon graduation.OHUB.Morehouse Innovation Lab.OHUB.CampusOpportunity Hub launched OHUB.Campus as a branded platform to provide colleges and universities and their respective administrators, faculty, and students with the exposure, knowledge, opportunities, connections, and tools to effectively launch a technology and startup ecosystem from the ground up, thereby connecting them to the global innovation, entrepreneurship, and investment economy. Campuses involved with OHUB.Campus can benefit from several opportunities, including:Access to OHUB’s brand and playbook for schools to launch a technology and startup entrepreneurship ecosystem on campus for students, alumni, and the surrounding communityAccess to a global network of startup ecosystems, entrepreneurship support programs, and fundingAccess to a global network of recruiters at technology and funded startup companiesAbility to establish and operate an official student chapter of Opportunity Hub on campusPartnership on the launch of an on-campus tech, entrepreneurship, and investment lab with a full-time program manager, student office hours, recruitment events, hackathons, and demo daysDelivery of a live webcast event each month to include exclusive presentations and conversations from diverse and inclusive technologists, recruiters, innovators, entrepreneurs, and investors on high-demand technology careers, high-growth startup entrepreneurship, and multigenerational wealth creation.Joint venture on offering OHUB’s African American Innovation, Entrepreneurship, and Investment courseCampus listing on the OHUB website and portal as a trusted safe space to learn, engage, and connect in the innovation economy regardless of age, race, ethnicity, gender, religion, or sexual orientationCollaboration on launching an industry-staffed coding, technical sales, and entrepreneurship bootcamp as a part of the school’s accredited curriculum or continuing education programs. Partnership with the local workforce development board is also an option.Opportunity to invest as a limited partner in OHUB’s partner seed-stage venture funds and portfolio companies.To fund the opportunities listed above, schools can pay for the program themselves from existing funds or fundraise through sponsorships, grants, or crowdsourcing. Alternatively, student chapters can sell memberships to community organizations and new startups. OHUB.Campus can cost between $25,000 and $2 million depending on which opportunities the school wishes to take advantage of. OHUB.365 Pivoting from live, in-person events in the wake of the COVID-19 pandemic, OHUB launched OHUB.365 as a portal to provide OHUB members with early exposure and introduction to edge technologies, skills, career pathways, startups, and funding in the fourth industrial revolution. Anchored by a growing library of subject matter expertise, members also receive a daily curated newsletter and webinar designed to position them to take advantage of opportunities traditionally leveraged by well-connected and privileged communities. Subject matter experts, respected authorities, recruiters, and ecosystem builders are invited to sign up to speak on OHUB.365 and invite their colleagues to do the same.Enterprises, investors, foundations, and developers are invited to sponsor student and professional memberships at $50 and $200 per year, respectively.OHUB.SXSWIn 2016, OHUB pioneered OHUB.SXSW and its HBCU@SXSW programming in partnership with SXSW, the largest interactive technology festival on the planet. Since then, over 550 Black and Latino or Hispanic college students have had the opportunity to attend this immersive experience, interview with technology companies, and secure well-paying summer internships or full-time jobs upon graduation. As an example, in 2019, NBCUniversal made 15 full-time offers, Deloitte made 18 full-time offers, Microsoft hired 21 for summer internships, and Mailchimp hired eight for summer internships. The median pay for summer internships was $20,000. The median starting salary for our full-time hires was $97,500. This yielded a growing economic impact of approximately $8,375,000 annually.In 2020, OHUB was preparing to bring 500 students to the fifth annual OHUB.SXSW. Due to COVID-19, however, the in-person experience was canceled and rescheduled as a virtual experience a week later, with over 1,100 students, young professionals, and entrepreneurs. For participating students, all travel, hotel, meals, conference registration was paid. Student OHUB members apply to be selected. The opportunity is supported by corporations, venture-backed startups, and foundations, including Deloitte, NBCUniversal, Accenture, Facebook, Google, KC Global Design, and the Kauffman Foundation. This has created over $10 million in new economic impact that continues to grow each year with the placement of new transformative talent into top companies. Hiring partners contribute between $10,000 and $250,000.OHUB.HIMSSThe Healthcare Information and Management Systems Society (HIMSS) is a global advisor and thought leader supporting the transformation of the health ecosystem through information and technology. HIMSS and OHUB have partnered to create OHUB.HIMSS as an innovative health equity initiative working to increase the pipeline of early technical and nontechnical Black and Latino or Hispanic talent at HIMSS member companies and beyond. Before COVID-19 cancellations, 50 college and university students were selected to attend HIMSS20, a global health conference and exhibition that brings together nearly 45,000 health information and technology professionals, clinicians, executives, and market suppliers from around the world to experience exceptional education, world-class speakers, cutting-edge products, and powerful networking opportunities. For students, all expenses would be paid to experience the conference and the many career pathways and opportunities that exist in health technology. Student OHUB members must apply to be selected. The opportunity is supported by corporations, venture-backed startups, and foundations, including Cigna, Express Scripts, and the HIMSS Foundation. These partners contribute between $10,000 and $250,000.OHUB.BlackInAI (OHUB@BAI) Black in AI (BAI) is a multi-institutional, transcontinental initiative creating a space for sharing ideas, fostering collaborations, and discussing initiatives to increase the presence of Black individuals in the field of AI. Through the support of its corporate partners, BAI sponsored 25 OHUB students in 2019 to attend its annual convening and workshop at the annual Conference on Neural Information Processing Systems in Vancouver. At the end of 2020, OHUB plans to provide support for sending their members to the Black in AI Workshop by awarding 100 scholarships to receive round-trip air travel, ground transportation, lodging, and meals. This event covers topics in deep learning, knowledge and statistical reasoning, machine learning, computer vision, artificial intelligence, computational neuroscience, and its various applications. It also delves into topics of fairness, ethics, and transparency in AI as it relates to people of color. Participants engage in deep conversation with others from the BAI global network, hear from world-renowned machine learning and AI researchers, and learn about various career opportunities that exist in this space. All expenses are covered, and Student OHUB members apply to be selected. The opportunity is supported by edge technology corporations that contribute between $5,000 and $200,000.OHUB Java bootcamp.Skills & trainingSince 2014, OHUB has been collaborating on immersive technology bootcamps for adult learners, recent college graduates, and youth to become proficient and employable as entry-level software developers. In 2015, OHUB launched the nation’s first software engineering and entrepreneurship workforce development program in partnership with leading coding schools and entrepreneurship support programs. This program became the model and framework for our nation’s TechHire initiative under the Obama White House and a national scholarship fund that trained over 300 undertapped Americans from low-income and socially disadvantaged communities to code in in-demand programming languages. They were subsequently placed in paid internships, apprenticeships, or full-time roles as entry-level software developers creating an economic output of over $15 million annually and growing.Today, through an effort called OHUB Futures under the leadership of Heather Hiles, OHUB is expanding its immersive training offerings nationwide and beyond via partnerships with employers, foundations, online training bootcamps, colleges and universities, and industry associations.Leif has joined forces with OHUB Futures to launch the first racial-equity-focused income share agreement (ISA) financing vehicle. The financing vehicle will provide funding to launch technology careers for thousands of Black Americans. The initial financing, targeting $50 million, endeavors to finance the education of 10,000 students per year on a recurring basis by leveraging the pay-it-forward philosophy of ISAs.In addition, OHUB Futures’ first HBCU partnership is a coding certificate program with Morehouse College and Durham, N.C.-based Momentum Learning. Branded Momentum@Morehouse, the program is a live, virtual, 12-week immersive coding program that offers expert-led training in Python/Django and career coaching, plus an extensive network to help students take up space in tech. A partnership between Morehouse College (an HBCU in Atlanta), Momentum Learning (a code school), and Opportunity Hub, Momentum@Morehouse prepares students to become professional software developers. Momentum instructors use a proven methodology to deliver top-rated code school training that exceeds industry standards. They teach students how to build and interact with in-demand software and web applications, including Python, JavaScript, React, HTML, CSS, Django, Git, and more. OHUB supplements the coding courses with “Culture of Work” courses that equip students with the soft skills and career readiness needed to meet the demands of the digitized economy. These courses give students the exposure and skills to navigate the technical interviewing and hiring process, accelerate their acclimation to a changing, more innovative and entrepreneurial workplace, and launch their journey toward financial independence and equitable multigenerational wealth creation in the future of work, fourth industrial revolution, and beyond. Anyone who is at least 18 years old with a high school diploma or equivalent can apply for Momentum@Morehouse, and all participants receive a Morehouse College certificate after successfully completing the program. Total tuition cost is $15,500 for the 12-week program, and alumni of Morehouse College, Spelman College, and Clark Atlanta University receive a $1,000 tuition discount. The program offers the options for participants to apply for a loan or defer payment via an income-sharing agreement. The average first-year starting salary for an immersive coding graduate is $65,000. Bootcamp graduates in their second and third year, on average, earn $77,000 and $90,000, respectively.Talent placementOHUB has a growing community of 5,000 university students and professionals at all levels representing nearly 400 colleges and universities, including nearly 100 historically Black colleges and universities (HBCUs), and they are actively seeking upwardly mobile career pathways in edge technology, the startup and venture ecosystem, and beyond. These individuals are uniquely positioned to accelerate the placement of talent at companies of all sizes. Through a collaboration with OHUB Futures, OHUB’s talent placement arm collaborates to place nontraditional talent into industry as well. Currently, OHUB works with over 50 companies to source and place hundreds of Black, Indigenous, and people of color (BIPOC) at all levels—including the board room—on contingency, pipelining, and success model bases.Going forward, OHUB is scaling to place thousands of engineers, operational professionals, and executives at leading technology, startup, and venture firms.OHUBKC Accelerator Portfolio company Boddle.Entrepreneurship support and startup accelerationSince 2013, OHUB has worked with hundreds of founders from socially disadvantaged backgrounds to start and build high-growth companies, access early-stage funding, and get to market.In 2019, OHUB expanded its entrepreneurship support programming and accelerator to Kansas City, Mo. via a public-private partnership with the Economic Development Corporation of Kansas City. With a starting operating budget of $1 million, over 2,000 people attended an OHUB monthly entrepreneurship event series, which provided a funnel to its one-day ideation and design thinking masterclass. From there, 60 product stage firms completed a six-month startup bootcamp. Ten finalists were selected to complete the NewMe pre-accelerator. Five venture-backable startups completed the accelerator and received a $50,000 investment and participated in a virtual demo day. Boddle, an education technology startup, recently received an investment of $375,000 and has grown from 1,000 users in March to over 50,000 users today. Musicbuk, a music tech startup, and Forefront, a human resources startup, received an investment from Techstars. Laundris, a sustainability tech AI platform, is now generating over $500,000 in annual recurring revenue and preparing for their seed round.Going forward in the Kansas City metro area region, OHUB is continuing its public-private partnership with the Economic Development Corporation of Kansas City while expanding its strategic partnerships with the Civic Council of Greater Kansas City and the Keystone Innovation District on the expansion and sustainability of its programming and economic output in the region. With an annual operating budget of $3 million, the program consists of:Tech and Startup Pitch Summit: An event to bring the nation’s top innovators, entrepreneurs, and investors of color together in Kansas City for two days of inspiration, networking, and deal flow.Talent Placement, Summer Internship, and Early Hires Initiatives: An effort where OHUB partners with local employers to create and/or expand their summer internships and early hiring programs to be more inclusive.Technical Sales Training: An eight-week training where a cohort of 15 to 20 students becomes proficient in technical sales techniques, processes, and best practices.Software Engineering Trainings: Trainings where OHUB and its portfolio company, Momentum Learning, work with corporations and cities to build custom training programs that teach the software engineering and lifelong learning skills required to address the industry demand for these roles. Programming languages taught are selected based on the immediate local or regional full-time employment opportunities.Monthly Entrepreneurship Event Series: Inspirational and tactical masterclass-style conversations and fireside chats with the world’s leading innovators, disruptors, startup founders, active angel and venture capital investors, and OHUB’s expert entrepreneurship support program partners.Immersive Ideation & Design Thinking Workshops: A one-day masterclass designed to take 100 founders through the agile process of developing a mindset and skillset to start a company from the ground up and navigate the science and best practices of being their best entrepreneurial selves as they seek to solve a business problem using edge technology and beyond.Startup Bootcamps: A program that helps founders identify critical decisions in their startup idea to help them navigate from idea to product market fit.Pre-accelerator for startups: The NewMe Pre-Accelerator program that takes 10 high-growth teams from product to market with a deeper focus on customer acquisition and fundraising over the course of a week. Participants receive a $5,000 grant.Accelerator for startups: A three-month program where five high-growth, product-based startups complete an immersive process by which they accelerate their opportunities to improve their product or service, increase customer acquisition, build diverse and inclusive teams, and raise follow on capital if applicable. Participants each receive a $50,000 investment, introductions to active investors, and access to market opportunities for customer acquisition.Nationally, OHUB plans to scale its entrepreneurship support programming and pre-accelerator to 100 cities over the next 10 years at a budget of $3 million per year per city.Keisha Knight Pulliam and Arian Simone of Fearless Fund.Capital formationIn 2019, 100 Black Angels & Allies Fund I was formed to institutionalize the thesis of investing in Black founders, accelerators, and funds. Its mission is to align smart, innovative, and skilled Black and ally capital to build a scalable and sustainable Black technology, startup entrepreneurship, and venture ecosystem. The fund is investing $1.5 million in venture funds managed by Black general partners that have a track record of increased performance, yet still experience implicit bias. These investments provide investors who have limited capital access with venture funds with larger minimums and pro-rata access to later-stage high-growth companies with the objective of further de-risking the principal investment. The fund also invested $1 million for a 7.5% stake in OHUB and $2 million in high-growth startup companies curated from the OHUB ecosystem. Limited partners invest a minimum of $50,000 to $5,000,000 in the fund.Cost and timeframe Funding and time frame of execution varies by program, service, or wholly owned subsidiary. These factors are noted under the program descriptions above.For the second half of 2020, OHUB is working toward raising $10 to $12 million to scale their work across all of their subsidiaries and programs to meet the current moment of COVID-19, the response to the ongoing racial injustice, and the realities that the national workforce faces. By 2021, OHUB plans to train 1,000 people at a cost of $15 million, pre-accelerate 500 high-growth firms at a cost of $2.5 million, accelerate 100 firms at a cost of $5 million, and invest in the seed rounds of 10 companies at $5 million.Key components and features For enterprises, OHUB is a supplier and partner of its diversity, equity, and inclusion Solutions (DEIS) to over 50 tech companies, major corporations, high-growth startups, and venture funds.For undertapped and socially disadvantaged communities, OHUB provides a membership model that provides access to curated content, programs, and resources designed to expose and connect aspiring technologists, professionals, innovators, entrepreneurs, and a new class of investors to opportunities in the fourth industrial revolution.For adult learners looking to reskill and upskill, OHUB provides immersive certificates in technical (coding, cybersecurity, distributed ledger) and nontechnical (sales, marketing, business) skills and helps them build their startup and access capital.For colleges and universities, with a special focus on HBCUs and minority-serving institutions (MSIs), OHUB delivers end-to-end technology, startup and venture ecosystem-building programs, initiatives, and revenue streams.For municipalities, as noted above, OHUB builds equity districts to create and sustain inclusive in-demand workforces and entrepreneurial ecosystems in the form of public-private partnerships to disrupt income inequality, poverty, and the racial wealth gap while increasing a region’s economic mobility and growth for all of its constituents.SuccessesWith its extensive and growing repertoire of programs, the Opportunity Hub is taking an innovative approach to close three major gaps: the racial representation gap in tech, funding and market gaps, and the racial wealth gap. OHUB has already begun the equity work that must be continued and duplicated in other U.S. cities to recover from the consequences of COVID-19. Taking steps to ensure that Black people don’t continue to feel the dire economic effects of structural racism will better position the country in COVID-19 recovery and beyond. ConsiderationsDuring the COVID-19 crisis and following the death of George Floyd, OHUB has seen a heightened increase in the demand of their work and services as a platform that centers racial equity, talent development, and access to capital. While funding from economic development organizations and foundations had been nascent, the sudden spike in the nation’s interest to dismantle white supremacy and achieve racial equity is a positive sign that this support will grow in even more places.Regarding their work, OHUB’s proximity to tech has allowed them to make the shift to virtual programming quite well. OHUB.365 is a new series that was born out of the COVID-19 era to provide daily content to continue their early exposure efforts. The in-person OHUB.SXSW conference was replaced with a virtual experience for participating students. Momentum@Morehouse launched in May 2020 as an effort to reskill workers who were displaced by COVID-19.In doing their work, the lessons that OHUB has learned are captured in this transparent statement by Rodney Sampson: “If we were non-Black doing this, we’d have raised billions by now and would have changed thousands of Black lives and communities. The difficult we do immediately. The impossible takes yet a second longer.” Sources and additional resourcesLinks to relevant press release, reports, published papers or the intervention website.Opportunity Hub: websiteThe Federal Reserve Bank of Kansas and Opportunity Hub: Building Entrepreneurship Ecosystems in Communities of Color reportOpportunity Hub: Company presentationOpportunity Hub: Ecosystem Building ProspectusOpportunity Hub: 100 Black Angles & Allies Fund I, LPOpportunity Hub: DEIS Talent AcquisitionOpportunity Hub: Kansas CityOpportunity Hub: Talent programsOpportunity Hub: 2019 Impact & 2020 Goals LetterOpportunity Hub: OHUB.365 Presenter InviteOHUB Foundation: Mission & WorkMomentum@Morehouse: websiteHIMSS Conference: HIMSS Partners with Opportunity Hub to Bring Students to HIMSS20Brookings: Black and Hispanic underrepresentation in tech: It’s time to change the equationBrookings: Could ‘mid-tech’ jobs elevate more people in non-coastal places?Reuters: Gap in U.S. Black and white unemployment rates is widest in five yearsBrookings: Why are Blacks dying at higher rates from COVID-19?Brookings: Rodney Sampson, Nonresident Senior FellowFast Company: Can Entrepreneurs Revive Martin Luther King’s Campaign For Economic Justice?Do you have a similar solution in your area? Is there another problem that you’re tackling in an innovative way that you’d like to share with a wider audience? Contact us at localrecovery@brookings.edu.Related Content U.S. EconomyBuilding racial equity in tech ecosystems to spur local recoveryDell Gines and Rodney SampsonJuly 23, 2020",https://www.brookings.edu/research/an-atlanta-organizations-mission-to-bring-racial-equity-to-the-tech-ecosystem/
python,Inside the Pentagon’s Secret Afghan Spy Machine,"The Pentagon’s top researchers have rushed a classified and controversial intelligence program into Afghanistan. Known as “Nexus 7,” and previously undisclosed as a war-zone surveillance effort, it ties together everything from spy radars to fruit prices in order to glean clues about Afghan instability.",https://www.brookings.edu/opinions/inside-the-pentagons-secret-afghan-spy-machine/
python,Think Bigger on North Korea,"While the world is fixated on Iraq and the Middle East, North Korea continues to pose at least as great a threat to Western security interests. Six-party talks with the North Koreans in Beijing have just showed that the Bush administration hasn’t yet found a way out of the nuclear crisis. Although negotiations appear likely to resume in a couple of months, their prospects for success seem poor.The basic dilemma is easy to understand. North Korea will not surrender its nuclear capabilities, which are among its only valuable national assets, unless offered a very good deal for giving them up. President Bush refuses to offer such a deal because he sees the North Korean demand as blackmail. He insists that before any talks about better diplomatic relations or economic interaction occur, North Korea first relinquish—with verification—a nuclear program it had pledged nine years ago to abandon completely. At most, Bush may, as an interim gesture, offer to sign a multilateral accord in which all six parties (the two Koreas, the United States, China, Japan and Russia) pledge not to attack each other.Meanwhile, in fits and starts North Korea continues its gradual progress toward a larger nuclear capability. Given the regime’s desperate economic straits, its erratic and eccentric and isolated regime and its threats last April that it might even export nuclear materials if circumstances got bad enough, this is extremely bad news.To the extent the Bush administration has a plan for addressing this crisis, it is a strategy of pressure. It insisted on the six-party negotiating format because that allows the other five parties all to insist that North Korea de-nuclearize. That kind of setting also deprives North Korea of its bluster and brinkmanship tactics.Related Books UpcomingNarco NoirBy Vanda Felbab-Brown 2025 UpcomingIran ReconsideredBy Suzanne Maloney 2025 UpcomingTo Rule the Waves: How Control of the World’s Oceans Determines the Fate of the SuperpowersBy Bruce Jones 2021For example, when the delegation from Pyongyang used the recent Beijing meeting to accuse the Bush administration of harboring aggressive designs on North Korea, Russia countered that it was confident the United States had no such intention. The Bush administration also has established a creative concept known as the proliferation security initiative, by which countries such as the United States, Japan, Australia, France and Germany make use of existing national laws to inspect North Korean ships in their waters—complicating North Korea’s efforts to smuggle illicit weapons, drugs and counterfeit currency.And the military card is still on the table in principle as well.But the Bush administration’s strategy is unlikely to work. Faced with gradual economic strangulation, North Korea’s stubborn and spiteful regime would probably again let its people starve—and perhaps consider selling dangerous weapons to terrorists—before crying uncle. Moreover, China, South Korea and Japan are far from ready to apply such a “python strategy.” China publicly criticized the United States for having an inflexible stance in last month’s talks. Michael E. O’HanlonDirector of Research - Foreign Policy Co-Director - Center for Security, Strategy, and Technology, Africa Security Initiative Senior Fellow - Foreign Policy, Center for Security, Strategy, and Technology The Sydney Stein, Jr. Chair TwitterMichaelEOHanlonJapan and South Korea both insisted on presenting a more conciliatory package of incentives to North Korea in Beijing than Washington was prepared to countenance. And none of our three key regional partners has any interest whatever in a military option at this point.Faced with this dilemma, we need to think bigger. We must offer much more to North Korea but demand far more in return. The goal should be to push North Korea, which has shown increasing interest in economic reform, to seriously attempt such reform—building on the precedents offered by China and Vietnam in the past two decades. If North Korea is willing and takes steps, such as cutting its conventional military forces, that are needed to give such a plan any hope of success, we can be generous in return. This would not be giving in to blackmail; it would be a form of assisted suicide for the Stalinist ways of the North Korean regime. Even if Kim Jong Il and his cronies survived the transition, their rule would be radically transformed.This plan would require the help of all six parties that are now part of the negotiating process. Chinese economists and technicians would teach the North Koreans how to carry out market reforms. Russia would reassure Kim Jong Il and his military commanders that intrusive arms control verification can be done without opening up the country to attack. Japan and South Korea would provide aid and investment; South Korea would also have to make at least modest cuts in its conventional forces in return for much deeper cuts in the oversized North Korean military.Beyond the nuclear and conventional military issues, North Korea would also agree to verifiable elimination of its chemical weapons and ballistic missiles. It would cease counterfeiting and drug trafficking. It would have to let all Japanese kidnapping victims leave and begin a human rights dialogue with the outside world. It would continue to abstain from terrorism and provocative actions against its neighbors.The United States would, for its part, ease trade sanctions immediately and ultimately lift them. It would, together with its regional partners and international financial institutions, provide at least $2 billion a year in aid to North Korea. The aid would not be in the form of cash (or new nuclear reactors), and would not be provided in one big dose but would be disbursed incrementally—while we watched to make sure North Korea was also holding up its end of the grand bargain.Diplomatic ties and security assurances leading to a full peace treaty would also be appropriate.Of course, this approach might well fail. North Korean leaders may, for example, believe they need nuclear weapons to deter the Bush administration from another preemptive action against another charter member of the axis of evil. But it would be a major mistake to act on that assumption before testing it. And if we try and fail, coercive policies may then become possible, as our regional partners will have a much harder time claiming that diplomacy has not yet been seriously attempted.",https://www.brookings.edu/opinions/think-bigger-on-north-korea/
python,Forum: Debating Bush’s Wars,"In the Winter 2007–08 issue of Survival, Philip Gordon argued that America’s strategy against terror is failing ‘because the Bush administration chose to wage the wrong war’. Survival invited former Bush speechwriter and Deputy Assistant to the President Peter Wehner and Kishore Mahbubani, Dean and Professor at the Lee Kuan Yew School of Public Policy in Singapore, to reflect on Gordon’s arguments. Their comments are available in the above PDF and Philip Gordon’s response is below.",https://www.brookings.edu/articles/forum-debating-bushs-wars/
python,Modeling with Data: Tools and Techniques for Scientific Computing,PREFACE,https://www.brookings.edu/articles/modeling-with-data-tools-and-techniques-for-scientific-computing/
python,The New Urban Demographics: Race Space & Boomer Aging,"America’s urban landscape is changing. The familiar distinctions between central cities and suburbs and between the growing Sunbelt and the more stagnant Frostbelt parts of the country are being complicated by new demographic trends, two in particular. The first trend is the sharp rise in immigration to the United States. Each year about one million people, predominantly Latin American and Asian in origin, arrive in the United States, most settling in urban areas. The second trend involves the baby-boomers. This large cohort of 76 million people?often termed “the pig in the python”?is now aging toward the tail of that python. Most boomers will not move but “age in place”?in the suburbs rather than in the city. Both these trends will have important effects on urban America.Beyond the “White-Black, City-Suburb” Typology For much of the postwar period, discussions of race and space in urban America revolved around black migration to central cities and “white flight” to the suburbs. The new immigration that is infusing many urban areas with new residents from a variety of backgrounds suggests the need for a new way of thinking about the demographic profiles of cities and suburbs.The impact of immigration is apparent from a glance at the central counties (those that contain a metropolitan area’s central-city population) showing greatest population gains during 1990?99. The two largest gainers, Maricopa County, home of Phoenix, and Clark County, home of Las Vegas, achieved most of their gains from domestic migration?migrants from other parts of the United States. Yet each of the next five central counties with the greatest population gains?those of the Los Angeles, Houston, San Diego, Miami, and Dallas metropolitan areas?registered a net loss of domestic migrants. Their gains came entirely from international migration and natural increase. Were it not for immigration, the population of these areas, and of several other large central counties, would register far smaller growth or outright declines. Indeed, areas that do not attract nearly as many immigrants?the central counties of Philadelphia, Pittsburgh, St. Louis, Cleveland, and Buffalo, among others?lost population over the 1990s.Because conventional city-suburb, black-white demographic profiles do not take adequate account of this new immigration, I offer a new typology of the nation’s large metropolitan areas, those with populations greater than 1 million (see table 1).Related Books Autonomous VehiclesBy Clifford Winston and Quentin Karpilow 2020 Making College WorkBy Harry J. Holzer and Sandy Baum 2017 Dream HoardersBy Richard V. Reeves 2017The typology begins with a Multiethnic High Immigration category of 12 metropolitan areas with high immigration and a significant Asian or Hispanic presence. The largest of these areas are New York, Los Angeles, Chicago, Washington, D.C., and San Francisco. Four other categories cover areas that have a primarily black minority presence and those that are mostly white; and within each, those growing at a relatively high pace and those growing only modestly. These four categories are: White-Black Fast-Growing (for example, Atlanta), White-Black Slow-Growing (Detroit), Mostly White Fast-Growing (Las Vegas), and Mostly White Slow-Growing (Pittsburgh).In metropolitan areas where white-black racial dynamics have been a historically important demographic dimension, the slow-growing areas tend to be in the Rustbelt (New Orleans being an exception), and the fast-growing areas are located in the Southeast, which has now begun to attract back significant numbers of African Americans. Mostly White Fast-Growing areas are located primarily in the West and Midwest (Orlando, Nashville, and West Palm Beach excepted). Mostly White Slow-Growing areas are located in the Northeast and Midwest (Louisville excepted). William H. FreySenior Fellow - Metropolitan Policy Program Table 2 gives the composite racial profiles for cities and suburbs in each of these five categories. Multiethnic High Immigration areas clearly have the greatest diversity both in their cities and in their suburbs although the suburbs remain majority white. Among the White-Black categories, the slow-growing areas show a lesser tendency toward black suburbanization.What these statistics show is that the conventional view of cities as being in decline and as having predominantly black populations fails to take into account recent changes in the urban scene. Today some of the nation’s fastest-growing cities are gaining population from domestic migration and are mostly white. And several large multiethnic metropolitan areas house “majority minorities” in their cities and may soon do so in their suburbs as well.The old “city-suburb” typology also fails to recognize heterogeneous growth patterns within the suburbs. Many inner and even middle suburbs are experiencing demographic dynamics similar to those of the cities. This is especially the case in some of the largest “melting pot” metropolitan areas. A look at the immigration and domestic migration dynamics for counties within the greater Los Angeles, San Francisco, and New York CMSAs (Consolidated Metropolitan Statistical Areas) shows that it is not just the inner counties whose gains are attributable solely to international migration. Of the 29 counties in the greater New York CMSA, fully 20 registered negative domestic migration during 1990?99 and achieved their only migration growth from immigration from abroad. The same goes for 4 of the 5 counties in the Los Angeles CMSA and for 7 of the 10 counties of the San Francisco CMSA.The heterogeneity of suburbs was already apparent in 1990. Figure 1 compares the Los Angeles, Atlanta, and Detroit metropolitan areas. The racially diverse suburbs of Los Angeles, with their infusion of immigrants and new ethnic minorities, contrast with the suburbs of Atlanta, which show moderate racial heterogeneity, and those of slower-growing Detroit, where historical racial antagonisms keep the divide between city and suburbs fairly sharp.Suburban growth patterns continue to favor the outer suburbs. Of the 30 counties that made the fastest gains via net domestic migration during 1990?99, most that are in metropolitan areas lie in the outer suburban reaches of fast-growing metropolitan areas such as Denver, Atlanta, Las Vegas, Dallas, and Houston. Even in the fast-growing, less dense portions of the country, an outer suburban residence remains popular.Aging Boomers, Cities, and SuburbsSince the baby-boom generation began entering grade school in the 1950s, it has been followed closely by marketers, policymakers, and political consultants. But the boomers’ sharp disengagement from the residential aspects of city life seems to have escaped the notice of urban watchers.The aging of the nation’s first suburban generation will increase substantially the number of households in their 50s and early 60s over the next 10 years. Early boomers?born between 1945 and 1955?will be making the transition from empty-nesters to preretirees. Many will retire from regular jobs. Some will leave their suburban homes. But most will “age in place” or perhaps make a local move.The late boomers?those born between 1955 and 1964?will still be in their prime career and prime earnings ages. Some will be looking to upgrade their housing, again in the same local area. They will have fewer children living at home than did earlier generations at the same age?and therefore more freedom in their location decisions.Some observers have expressed the hope that these huge boomer generations might be a source for central-city revival. But the hope seems unrealistic, given the current location of this “suburban generation.” With the exception of Hispanics, baby-boomers, now in the 35-54 age groups, are less likely to reside in the city than either today’s elderly or adults now in their 20s and early 30s.In fact, elderly growth patterns during the 1990s show that the “graying of the suburbs” is well under way. Of the 30 counties with fastest-gaining elderly populations over the 1990s, most are either in nonmetropolitan areas or in middle or outer suburban counties of fast-growing metropolitan areas. Much of this simply reflects the aging-in-place of elderly who have moved to these fast-growing metropolitan areas during their lifetimes.The projected gains in elderly over the next 25 years will occur in the Mostly White Fast-Growing and White-Black Fast-Growing areas discussed earlier. While some of this growth will be attributable to retirement migration to high amenity areas of the Mountain West and South, most will result from the aging-in-place of boomers who relocated there during their working lives or at least before retirement.This suggests somewhat different patterns of growth for the new immigrant minorities than for largely white or white and black baby-boomers. Much of the boomer growth and, hence, the projected elderly gains will be in the suburbs of major metropolitan areas and in regions of the country that are not capturing new immigrant minorities. Within metropolitan areas the better-off and healthy “yuppie elderly” will tend to locate on the periphery, and the more disadvantaged segments of the older population will reside closer in.Central cities and inner suburbs in metropolitan regions that have suffered economic and demographic declines in recent decades will continue to house disproportionate numbers of the nation’s disadvantaged elderly?older elderly people, widows and widowers, female-headed households, those with incomes below or near the poverty level and relatively high levels of disability. As they continue to age in place, they will pose special challenges for local institutions that are often the most financially strapped.But although the aging of the baby-boomers will have different effects for cities and suburbs, dealing with the social services, health care, and transportation needs of a faster-growing senior population will prove a challenge to cities and suburbs alike.New Forces at WorkThe advent of new immigrants and the aging of the baby-boomers will surely complicate both urban and suburban race-ethnic and aging demographic dynamics.The demographics of Los Angeles provide just one example of how the two countervailing trends could operate. As a result of the successive outmigration of whites, juxtaposed with the continued waves of immigration of new ethnic minorities, Los Angeles County’s elderly population is still majority white, its working-aged population is only about one-third white, and its child population is predominantly Hispanic and other racial and ethnic groups. Reflecting their age, the growing racial and ethnic groups in Los Angeles will be concerned with issues of affordable housing, good schools, and neighborhoods conducive to the raising of children. And, reflecting their age, whites are likely to be more concerned with health and social support services for an aging dependent population. Whether the same kind of “racial generation gap” will occur in other melting pot metropolitan areas remains to be seen.Whatever else the new urban population profiles show, the old models of dealing with cities and suburbs will need to be revised to adapt to new demographic forces in America today.",https://www.brookings.edu/articles/the-new-urban-demographics-race-space-boomer-aging/
data science,Building Data Science Infrastructure,"Presented by Caitlin Hudon – Lead Data Scientist at OnlineMedEd
Before AI, before machine learning and pipelines, and before dashboards and BI, an organization starts with a pile of data, some business questions, and a few ideas on how to connect the two — a greenfield, and an entry point for data science.
Answering business questions and turning raw data into insights, models, and products means more than just writing code and doing analysis. A successful data science team needs tools, a communication strategy, thoughtful infrastructure, and a plan to deliver on their goals. This talk will cover how to tackle greenfield data science challenges from the perspective of the first data science hire in an organization, and how to build data science infrastructure from the ground up.",/videos/building-data-science/00007ZK3AGMACZS/
data science,What Indeed’s Job Market Data can tell us about Demographics and Trends in Data Science,"Presented by Chris Lindner – Manager, Product Science at Indeed
Over the past decade, data science has exploded as a lucrative, high-demand career. During this time, we’ve seen rapid expansion in both the demand for data scientists, and for the number of individuals trying to get into the field. But what exactly does the title “data scientist” even mean? Who are these “data scientists,” and where do they come from? Are we becoming overly flooded with data science candidates? What emerging trends do we see as more and more jobseekers enter the market to meet this growing demand? Indeed is the world’s #1 job site. Using our data on job postings, searches, and resumes, I’ll try to explore the answers to some of these questions, and paint a picture of what the job market looks like today, and where it is going in 2020 and beyond.",/videos/what-indeeds-job/00000LE9TFJBFPZC/
data science,ODSC East 2019 (Open Data Science Conference),"ODSC East 2019
Royalties for this video set help fund the ODSC Grant Award for open source data science projects.
The Open Data Science Conference has established itself as the leading conference in the field of applied data science. Each ODSC event offers a unique opportunity to learn directly from the core contributors, experts, academics and renowned instructors helping shape the field of data science and artificial intelligence
Presentations cover not only data science modeling but also the languages and tools needed to deploy these models in the real world such as TensorFlow, MXNet, scikit-learn, Kubernetes, and many more.
Our conferences are organized around focus areas to ensure our attendees are at the forefront of this fast emerging field and current with the latest data science languages, tools, and models. You’ll find in our East 2019 video catalog some of our most popular focus areas.
Please see our table of contents for a full list of videos",/videos/odsc-east-2019/9780136746874/
data science,ODSC East 2018 (Open Data Science Conference),"ODSC
The Open Data Science Conference has established itself as the leading conference in the field of applied data science. Each ODSC event offers a unique opportunity to learn directly from the core contributors, experts, academics and renowned instructors helping shape the field of data science and artificial intelligence
Presentations cover not only data science modeling but also the languages and tools needed to deploy these models in the real world such as TensorFlow, MXNet, scikit-learn, Kubernetes, and many more.
Our conferences are organized around focus areas to ensure our attendees are at the forefront of this fast emerging field and current with the latest data science languages, tools, and models. You‚Äôll find in our East 2018 video catalog some of our most popular focus areas including:
Deep Learning and Machine Learning
Over the last 5 years, we have seen incredible advances in the field of data scientist thanks to breakthroughs in neural networks, transfer learning, reinforcement learning, and generative adversarial networks (GANs) to name a few. With the advent of Google Voice, Alexa, and other voice assistants, presentations on enabling technologies like NLP, RNNs, and LSTM are popular. Some session to note:
Distributed TensorFlow using KubernetesDeep Learning Pipelines for Big ImagesMachine Learning and Natural Language Processing for Detecting Fake NewsLong-Term Time Series Forecasting with Recurrent Neural NetworksA Breakthrough for Natural LanguageTransfer Learning: Applications for natural language understandingChallenges and Opportunities in Applying Machine LearningEffective Transfer Learning for NLPDeveloping Machine Learning Solutions with Plugin Machine Intelligence for PDIDistributed Tensorflow: Scaling Your Model Training
Core Data Science and Data Visualization
As data science advances at a rapid pace, core skills are more important than ever. Our sessions range from beginner to advanced level for core topics. Additionally, data and models need to be actionable and data visualization remains a key skill in any data scientist's toolkit. Some session of note include:
Bayesian Statistics Made SimpleGradient Descent, DemystifiedComparing Models Using Resampling and Bayesian MethodsNext Generation Indexes For Big Data EngineeringProbabilistic Programming with PyMC3Racial Bias in Facial Recognition SoftwareVisualization throughout the Data Science Workflow:Datafy All The Things
Data Science, Management, And Business
Data science is permeating every industry as adoption gathers pace. The management and practice of data science will become increasingly strategically important to all industries including finance and healthcare. Hear from leading experts on important topics including:
Blockchain and AI: future data systems must be built differentlyMarketing in a Machine Learning WorldThe Adoption of AI in Business: Opportunities and Challenges:Winning with AI: What's working, what needs work?Building an effective AI practiceApplied Finance: The Third CultureMachine Learning Powers Better Decisioning in Financial ServicesAI and Data Science in InvestmentAI and Big data in Medicine: Trust, Transparency, and TransformationAlgorithmic Transparency and Health Care: Where do we go from here?What will you do with democratized health data?A Physician-Data Scientist Grand Vision: A Virtual Medical Oracle
Thought Leadership | Keynotes
Data science and artificial intelligence are helping shape the future of business and society. Thoughtful leadership is essential and ODSC East was honored to host a few of the field's leading lights including Cathy O‚ÄôNeil, author of Weapons of Math Destruction, Drew Conway, one of the most well-known data scientists in the world, Gary Marcus, an award-winning Professor, and Kirk Borne, data scientist and executive advisor at Booz-Allen Hamilton. Thought-provoking presentations include:
Catherine O'Neil | Weapons of Math Destruction: Separating Data Facts and OpinionDrew Conway | Building a Data Science CompanyKirk Borne | Current and Future Trends in AI, Machine Learning, and Data ScienceGary Marcus | The Social Disease Known as Hype in AI and How to Separate What AI can do and What People Wish it Would do
Please see our table of contents for a full list of videos",/videos/odsc-east-2018/9780135432792/
data science,"Programming Skills for Data Science: Start Writing Code to Wrangle, Analyze, and Visualize Data with R, First Edition","The Foundational Hands-On Skills You Need to Dive into Data Science

“Freeman and Ross have created the definitive resource for new and aspiring data scientists to learn foundational programming skills.”
–From the foreword by Jared Lander, series editor
Using data science techniques, you can transform raw data into actionable insights for domains ranging from urban planning to precision medicine. Programming Skills for Data Science brings together all the foundational skills you need to get started, even if you have no programming or data science experience.

Leading instructors Michael Freeman and Joel Ross guide you through installing and configuring the tools you need to solve professional-level data science problems, including the widely used R language and Git version-control system. They explain how to wrangle your data into a form where it can be easily used, analyzed, and visualized so others can see the patterns you've uncovered. Step by step, you'll master powerful R programming techniques and troubleshooting skills for probing data in new ways, and at larger scales.

Freeman and Ross teach through practical examples and exercises that can be combined into complete data science projects. Everything's focused on real-world application, so you can quickly start analyzing your own data and getting answers you can act upon. Learn to
Install your complete data science environment, including R and RStudioManage projects efficiently, from version tracking to documentationHost, manage, and collaborate on data science projects with GitHubMaster R language fundamentals: syntax, programming concepts, and data structuresLoad, format, explore, and restructure data for successful analysisInteract with databases and web APIsMaster key principles for visualizing data accurately and intuitivelyProduce engaging, interactive visualizations with ggplot and other R packagesTransform analyses into sharable documents and sites with R MarkdownCreate interactive web data science applications with ShinyCollaborate smoothly as part of a data science team
Register your book for convenient access to downloads, updates, and/or corrections as they become available. See inside book for details.",/library/view/programming-skills-for/9780135159071/
data science,Utilizing Data and Data Science to Optimize Tune In,"Presented by Diane Leung, Principal, Analytics Innovation at Altman Vilandrie & Company
Measuring and optimizing tune-in is critically important for the media and entertainment industries. This discussion focuses on best practices for utilizing data and machine learning to optimize tune-in on national linear inventory. In order to do this properly, advertisers need to unify their marketing ecosystem, design a holistic measurement approach, and break down barriers for closed-loop, incremental measurement. In this session you will learn how to: 1) Create a framework for utilizing data and machine learning to maximize tune-in and 2) Overcome analytical obstacles created from fragmented and incomplete data.",/videos/utilizing-data-and/00000TSOQB75GW8K/
data science,ODSC West 2018 (Open Data Science Conference),"Royalties for this video set help fund ODSC community initiatives such as grants to open source projects, our diversity program, student travel grants, and other initiatives.
The Open Data Science Conference has established itself as the leading conference in the field of applied data science. Each ODSC event offers a unique opportunity to learn directly from the core contributors, experts, academics and renowned instructors helping shape the field of data science and artificial intelligence Presentations cover not only data science modeling but also the languages and tools needed to deploy these models in the real world such as TensorFlow, MXNet, scikit-learn, Kubernetes, and many more. Our conferences are organized around focus areas to ensure our attendees are at the forefront of this fast emerging field and current with the latest data science languages, tools, and models. You’ll find in our East 2018 video catalog some of our most popular focus areas including:
Deep Learning and Machine Learning
Over the last 5 years, we have seen incredible advances in the field of data scientist thanks to breakthroughs in neural networks, transfer learning, reinforcement learning, and generative adversarial networks (GANs) to name a few. With the advent of Google Voice, Alexa, and other voice assistants, presentations on enabling technologies like NLP, RNNs, and LSTM are popular. Some session to note:
OS for AI: How Serverless Computing Enables the Next Gen of ML—Jon Peckpomegranate: Fast and Flexible Probabilistic Modeling in Python—Jacob SchreiberData Wrangling to Provide Solar Energy Access Across Africa—Brianna Schuyler, PhDThe past, present, and future of Automated Machine Learning—Randy Olson, PhDMinimizing and Preventing Bias in AI—Frances HaugenDeep Learning on Mobile—Anirudh KoulState of the Art Natural Language Understanding at Scale—David Talby, PhDLatest Developments in GANS—Seth WeidmanHow to Reason About Stateful Streaming Machine Learning Serving—Lessons from Production—Patrick BoueriAn Introduction to Active Learning—Jennifer Prendki, PhDHow to use Satellite Imagery to be a Machine Learning Mantis Shrimp—Sean Patrick Gorman, PhD
Core Data Science and Data Visualization
As data science advances at a rapid pace, core skills are more important than ever. Our sessions range from beginner to advanced level for core topics. Additionally, data and models need to be actionable and data visualization remains a key skill in any data scientist’s toolkit. Some session of note include:
Panel: Visual Search: The Next Frontier of Search—Clayton MellinaVisualizing Vectors: Basics Every Data Scientist Should Know—Jed CrosbyRevolutionizing Visual Commerce—Robinson PiramuthuScaling Interactive Data Science and AI with Ray—Richard LiawThe Platform and Process of Agile Data Science—Sarah Aerni, PhDThe AI Engineer: A Foot in Two Worlds—Guy Royse
Data Science, Management, And Business
Data science is permeating every industry as adoption gathers pace. The management and practice of data science will become increasingly strategically important to all industries including finance and healthcare. Hear from leading experts on important topics including:
Managing Effective Data Science Teams—Conor JensenA Manager’s Guide to Starting a Computer Vision Program—Ali Vanderveld, PhDWord Play: Understanding the Mechanics and Business Value of Speech Technologies—Omar TawakolJust How Much Data Is Required to Make Autonomous Vehicles Truly Road-Ready?—Alexandr WangHow to Democratize Artificial Intelligence in Your Business—Olivier BlaisReality Check: Beyond the Hype. Real Companies Doing Real Business Getting Real Value with AI—Alyssa RochwergerAn Ethical Foundation for the AI-driven Future—Harry GlasserBest Practices for Deploying Machine Learning in the Enterprise—Robbie AllenGreatest hurdles in AI proliferation in Education—Varun Arora10 Things I Learned Deploying AI into Human Environments—Cameron Turner
Thought Leadership &verbar; Keynotes
Data science is permeating every industry as adoption gathers pace. The management and practice of data science will become increasingly strategically important to all industries including finance and healthcare. Hear from leading experts on important topics including:
Data Science and Open-Source Education for the Enterprise—Zachary Sean BrownTurning Machine Learning Research into Products for Industry—Reza Bosagh ZadehAI—Disruption for the Marketing World—Luc Dumont
Please see our table of contents for a full list of videos.",/videos/odsc-west-2018/9780136526476/
data science,Improving the Worlds Largest Online Grocery Catalog: A Data Science Story,"Presented by Ishant Nayer, Sr Data Scientist at Instacart
Being a data-driven company, Instacart realizes the power of good quality data. While trying to maximize the efficiency of data consumption by all work streams such as recommendation systems and availability systems, we are trying to make our Catalog the best in the world by delivering precise information to all our end-users including shoppers and consumers.
This session will cover the following areas:1. How we used data science to auto-detect inconsistencies in the data attributes of millions of items comprising the Catalog, in real-time.2. How we defined and utilized North Star metrics to optimize data quality of our Catalog3. Different approaches being used to deliver a great customer experience.",/videos/improving-the-worlds/00000I9SFX47SIE/
data science,What all policy analysts need to know about data science,"Alex EnglerRubenstein Fellow - Governance Studies  Twitter@AlexCEnglerConversations around data science typically contain a lot of buzzwords and broad generalizations that make it difficult to understand its pertinence to governance and policy. Even when well-articulated, the private sector applications of data science can sound quite alien to public servants. This is understandable, as the problems that Netflix and Google strive to solve are very different than those government agencies, think tanks, and nonprofit service providers are focused on. This does not mean, however, that there is no public sector value in the modern field of data science. With qualifications, data science offers a powerful framework to expand our evidence-based understanding of policy choices, as well as directly improve service delivery.To better understand its importance to public policy, it’s useful to distinguish between two broad (though highly interdependent) trends that define data science. The first is a gradual expansion of the types of data and statistical methods that can be used to glean insights into policy studies, such as predictive analytics, clustering, big data methods, and the analysis of networks, text, and images. The second trend is the emergence of a set of tools and the formalization of standards in the data analysis process. These tools include open-source programming languages, data visualization, cloud computing, reproducible research, as well as data collection and storage infrastructure.Source: Alex Engler/The University of ChicagoPerhaps not coincidentally, these two trends align reasonably well with the commonly cited data science Venn diagram. In this diagram, data science is defined as the overlap of computer science (the new tools), statistics (the new data and methods), and critically, the pertinent domain knowledge (in our case, economics and public policy). While it is a simplification, it is still a useful and meaningful starting point. Moving beyond this high-level understanding, the goal of this paper is to explain in depth the first trend, illuminating why an expanded view of data and statistics has meaningful repercussions for both policy analysts and consumers of that analysis.Traditional evidence-building for policy analysisUsing data to learn about public policy is not at all new. The origins of the social sciences using statistical analysis of observational data goes back at least to the 1950s, and experiments started even further back. Microsimulation models, less common but outsized in their influence, emerged as the third pillar of data-driven policy analysis in the 1970s. Beyond descriptive statistics, this trifecta—experiments, observational statistical analysis, and microsimulation—dominated the quantitative analysis of policy for around 40 years. To this day, they constitute the overwhelming majority of empirical knowledge about policy efficacy. While recent years have seen a substantial expansion in the set of pertinent methods (more on that below), it is still critical to have a strong grasp of experiments, observational causal inference, and microsimulation.ExperimentsSince public policy can’t be conducted in a laboratory, experiments are rare in policy studies. Experiments require random assignment, which for policy means a benefit or program is made available randomly to some people and not to others—hardly a politically popular strategy. Many would also say it is ethically questionable to do this, though randomized experiments have taken firm root in medicine, sacrificing fairness in the short term for progress in the long term. Regardless of the political and ethical barriers, they do happen. Experiments are most often supported by nonprofits or created by an accident of governance, and can produce relatively rigorous results, compared to the other methods discussed here.Perhaps the most famous experiment in modern public policy is that of the Oregon Medicaid expansion. When Oregon moved to expand access to Medicaid in 2008 (before the Affordable Care Act), the state quickly realized that it could not afford to cover all the individuals eligible under the loosened criteria. Opting to randomly select which residents would be able to receive benefits, Oregon officials created the perfect circumstances for researchers to compare recipients of Medicaid with non-recipients who were otherwise very similar. Professors Katherine Baicker and Amy Finkelstein led the research efforts, resulting in extensive evidence that Medicaid improved some health outcomes and prevented catastrophic medical expenses, while also increasing health-care utilization and costs. Signaling a growing interest in this approach, the recent Nobel Prize in Economics recognized three scholars who have taken experiments (sometimes call randomized control trials, or RCTs) into the developing world to examine how to best tackle global poverty.Statistical analysis of observational dataDue to the financial and political difficulties that experiments present, they remain rare, and much more research is based on the statistical analysis of observational data. Observational data refers to information collected without the presence of an explicit experiment—it comes from surveys, government administrative data, nonprofit service delivery, and other sources. Usually by obtaining and combining several datasets, researchers look for various opportunities to examine the causal effects of policy changes with statistical methods. These statistical methods, broadly called causal inference statistics (or quasi-experiments), take advantage of differences within populations, or policy changes over time and geography to estimate how effective a service or intervention is.Individually, the strength of the evidence from a single study is limited. (This is true in any field, and it suggests prudence when changing your beliefs based on results from one study.) However, since observational data is far easier to gather and analyze than experimental data, it is possible to find many opportunities to re-examine the same policy questions. Eventually, it’s possible to examine many papers on the same subject, called a meta-analysis. Meta-analysis of observational studies have convincingly argued that increased school spending improves student outcomes, gun access leads to higher risk of suicide and homicide, and that taxes on sugary beverages are associated with lower demand for those beverages.Although at times difficult to interpret, this slow accumulation of many observational analyses by different research groups often becomes the most informative and trustworthy source of information about potential policy changes.MicrosimulationAlthough microsimulation is a lesser-known type of modeling, it remains a critical one. The news is frequently covered in estimates from microsimulation methods, such as how effective taxes would change under the Tax Cuts and Jobs Act and how many people would lose health insurance under the curtailing of the Affordable Care Act. Even a substantial part of the (in)famous Congressional Budget Office scoring of the cost of federal legislation employs microsimulation.The Urban Institute-Brookings Institution Tax Policy Center model is perhaps the easiest to understand intuitively. The model starts with a sample of anonymized administrative data from the Internal Revenue Service, which contains lots of information about taxpayers that is specific to each person. (This puts the “micro” in microsimulation.) The model itself then does the same thing as online tax preparation software: It runs through the rules of the tax code and calculates how much this person should be paying in taxes. However, the model contains many different knobs that can be turned and switches that can be flicked, each one changing something about the way the tax code works. By altering some of these inputs, the model creates a simulation, that is, an alternative possible outcome from the real world of tax policy.These models are highly complex, and usually take years to build. They also require a lot of information about how a set of public policies are currently affecting a population, so the data typically comes from government administration records. However, once they are built, they offer a quick and flexible lens into potential policy changes. In reality, the behavioral consequences—how people and firms react to new policy—are large enough that few experts are ever really convinced that estimates from these models are precisely correct. That said, microsimulation methods can ground policy discussions to reasonable predictions, make assumptions explicit, and give a reasonable sense of what complex and interacting policy changes might do. Compared to letting pundits invent numbers out of thin air, microsimulation offer a dramatically more rigorous approach to estimating policy outcomes.The expanded methods of data science for policy analysisNothing discussed above falls outside the field of data science. These approaches all use data, programming, and statistics to infer meaningful conclusions about the world. Still, the term “data science” has some value, as it connotes a broader set of methods and data types than is traditional to the field of policy analysis. While many of these methods have existed for a long time, the proliferation of new and diverse data sources means this expanded toolkit should be more widely understood and applied by policy analysts. Many of the methods detailed below fall into the field of machine learning, but in this case, that terminology complicates the issue without adding much clarity.Predictive analyticsThere is a growing recognition that many government and nonprofit services can be improved with predictive analytics. In Chicago, predictive models are used to reduce the exposure of young children to lead paint, which has extensive and permanent health consequences. Before this effort, and still in most places across the United States, exposed lead paint is typically only discovered after children fall ill.The Chicago model uses historical inspection data to find correlations between exposed lead paint and other information (like the age of the buildings, when they were last renovated, if they have been vacant, as well as demographic data). This model can then be used to evaluate the level of risk of lead paint in homes that are going to accommodate newborn children. Using those predictions, the Chicago Department of Public Health can more strategically prioritize lead paint inspections, saving many children from hazardous exposure.This is a generalizable approach for service providers who have a valuable intervention, limited resources, and uncertainty in where their investments would be most beneficial. As another example, the Center for Data Insights at MDRC—a nonprofit, nonpartisan education and social policy research organization—is exploring how to use prediction modeling to better allocate employment services to former inmates. (Disclosure: I am a data science consultant on this project.) If there is trustworthy historical data and an opportunity to affect who gets an intervention, predictive analytics can be highly beneficial by getting services delivered to those who need it most.ClusteringIn public policy, subgroups of a larger population can be very important. Some students score highly on tests, while other score poorly. Some people earn a lot of money from their jobs, while others earn very little. However, although it is tempting to think of groups as separated along a single variable, like the examples above, this is infrequently the case. Some people may earn little money from their jobs, but are in fact in graduate school, have highly educated parents with a strong social support system, suggesting that their income potential is quite high. In some cases, they may be more similar to people earning lots of money than those who earn little but do not have those other supports.Clustering methods allow for the discovery of these underlying groups across many variables that might otherwise remain hidden or avoid our qualitative intuition. The Pew Research Center has demonstrated this by using clustering methods to examine our assumptions about the political spectrum. Pew researchers applied clustering methods to a survey with 23 questions about political opinions. They discovered that the traditional liberal-moderate-conservative spectrum does not effectively compass the many dimensions of political views that Americans hold. Instead, they argued for seven distinct political subgroups. As just one difference, this more nuanced political analysis notes two groups of conservatives: one more socially conservative, but also critical of Wall-Street; and another more socially moderate, pro-immigration, but also pro-Wall Street.This is closer to how the world really works—subgroups are complex and nothing is unidimensional. It’s imperative to consider how many variables may be interacting to define the most meaningful differences in whatever populations are being analyzed.Big dataSometimes, though certainly not always, simply having more data enables better or different policy analysis. Over the past 12 years, a joint academic initiative at MIT Sloan and Harvard Business School has been using online prices to measure macroeconomic indicators. By scraping data from the internet, the Billion Prices Project has collected the prices of 15 million items from over a thousand retailers. This massive dataset has enabled them to create measures of inflation in 20 countries, updated on a daily basis. For the sake of comparison, the Bureau of Labor Statistics’ (BLS) Consumer Price Index is monthly. Although there are many challenges to this new approach, it’s worth keeping in mind that the traditional process used by the BLS (government employees surveying or physically examining prices) is far more expensive, complicated by its own problems (e.g., growing survey non-response), and painstakingly slow.While big data can offer new insights, there are important statistical differences when analyzing big data. Most notably, it is generally harder to get data that accurately represents the whole of a population (like a country or a state). Cloud computing and modern software may easily enable analyzing multi-billion row datasets, but that makes it no easier to know who the data is relevant to. Phone records can illuminate strategies to improve traffic patterns, but does it overlook people without mobile phones? Credit card transactions can reveal lifestyle differences across socio-economic groups, but what could be missing without seeing cash transactions and cash-only consumers? This remains a large problem for effectively using big data that is not originally meant for social science (sometimes called “found data”). As a result, it’s a priority to continue the development of methods that can adjust these datasets to be representative and accurate, especially since this new data can offer so much.Text analysisModern text analysis, or natural language processing, offers new ways to glean meaningful insights into the huge bodies of writing that societies produce. For example, consider the impressions that a community has of its law enforcement officers. Trust in the legitimacy of a police force can lead to more lawful behavior, as well as community collaboration to solve and reduce crimes. However, community impressions of police can be hard to measure. This is why the Urban Institute turned to Twitter. Researchers at Urban’s Justice Policy Center analyzed the sentiment of 65 million tweets, finding spikes in negative sentiment after violent police-citizen interactions. It’s worth nothing that this analysis is affected by the big data considerations detailed above.In another instance, my colleagues in Brookings’s Metropolitan Policy Program looked for overlapping patterns in the text of job descriptions and AI patents. This allows them to create a quantitative estimate of how good AI might be at various job tasks, and thereby, how potentially automatable those jobs might be. This approach creates a new way to reason about the effects of automation that is less dependent on the qualitative judgement of experts.Network analysisIn their book, “Network Propaganda,” three researchers of Harvard’s Berkman-Klein Center created networks of online media sources, like The New York Times and Fox News. They then measured the sources’ relationships to one another with the hyperlinks in their news content, as well as the social media sharing patterns of their audience. Critically, their research has shown how isolated and self-amplifying far-right media sources have become, leading them to grow more extreme and less tethered to the truth. This is the exact type of insight that network analysis can help deliver: Around whom is the network most dependent? Who is on the fringes and sidelines? How are relationships between actors changing?While the internet and social media has created huge networks of people, any group of things with relationships to one another can be considered a network and analyzed as such. The states have long been considered laboratories of democracy, where experimental policies can be tested and successful ones shared. This also can be conceived of as a network, with states being connected to one another through the diffusion of similar legislation. Recent research of this kind has provided further evidence of California’s status as a leader in policy innovation. This might be unsurprising, but the same research also highlights Kentucky as the most influential state for the diffusion of public policy from the 1960s until California's emergence in the mid-1990s.Image analysisImage data has proliferated in recent years, originating in everything from cell phones, traffic cameras, smart devices, and even constellations of new satellites. In many parts of the word, especially poor countries, it can be very hard to develop an accurate understanding of poverty—but analyzing image data can help. Unsurprisingly, knowing where impoverished people are is vital to targeting services and investments to help improve their socio-economic outcomes. This is why the World Bank has been developing methods to use high-definition satellite data to create geographically specific measures of poverty, especially relevant to the 57 countries with almost no survey data on poverty. Data science methods can look at satellite imagery and recognize cars, identify roofing materials, distinguish between paved and unpaved roads, and measure building height and density. In turn, these new variables can be used to estimate fairly accurate poverty measures that are substantial improvements over outdated (or nonexistent) estimates.Satellite data has also been used to monitor international conflicts and as evidence of human rights abuses. Other efforts have proposed using social media photos of political protests to measure their size and degree of violence, though this is likely not ready for implementation. Currently, image analysis is largely limited to facial and object recognition; it is not close to genuine understanding of photos. Still, as imagery proliferates and related modeling techniques improve, this data will offer powerful new ways to examine the state of the world.Why data science matters to public policy and governanceEvaluating data is becoming a core component of government oversight. The actions of private companies are more frequently in databases than file cabinets, and having that digital information obscured from regulators will undermine our societal safeguards. Government agencies should already be acting to evaluate problematic AI-hiring software and seeking to uncover biases in models that determine who gets health interventions. As algorithmic decision-making becomes more common, it will be necessary to have a core of talented civic data scientists to audit their use in regulated industries.Even for public servants who never write code themselves, it will be critical to have enough data science literacy to meaningfully interpret the proliferation of empirical research. Despite recent setbacks—such as proposed cuts to evidence-building infrastructure in the Trump administration’s budget proposal—evidence-based policymaking is not going anywhere in the long term. There are already 125 federal statistical agencies, and the Foundations of Evidence Based Policymaking Act, passed early last year, expands the footprint and impact of evidence across government programs.“Even for public servants who never write code themselves, it will be critical to have enough data science literacy to meaningfully interpret the proliferation of empirical research.”Further, the mindset of a data scientist is tremendously valuable for public servants: It forces people to confront uncertainty, consider counterfactuals, reason about complex patterns, and wonder what information is missing. It makes people skeptical of anecdotes, which, while often emotionally powerful, are not sufficient sources of information on which to build expansive policies. The late and lauded Alice Rivlin knew all this in 1970, when she published “Systemic Thinking for Social Action.” Arguing for more rigor and scientific processes in government decision-making, Rivlin wrote a pithy final line: “Put more simply, to do better, we must have a way of distinguishing better from worse.”How to encourage data-scientific thinking and evidence-based policiesThe tools and data to distinguish better from worse are more available than ever before, and more policymakers must know how to use and interpret them. A continued expansion of evidence-based decision-making relies on many individuals in many different roles, adopting practices that encourage data-scientific thinking. Managers in government agencies can hire analysts with a rigorous understanding of data in addition to a background in policy. They can also work to open up their datasets, contributing to Data.gov and the broader evidence-based infrastructure. Grant-making organizations have a critical role, too. They should be mandating an evaluation budget—at least 5% of a grant—to collect data and see if the programs they are funding actually work. When they fund research, it should require replicable research and open-data practices.Related Content Technology & InnovationThe case for AI transparency requirementsAlex EnglerWednesday, January 22, 2020 Technology & InnovationWhat is artificial intelligence?Darrell M. WestThursday, October 4, 2018 Technology & Innovation5 questions policymakers should ask about facial recognition, law enforcement, and algorithmic biasRashawn RayThursday, February 20, 2020For policy researchers looking to expand their sense of what is possible, keep an eye on the data science blogs at the Urban Institute and the Pew Research Center, which get into the weeds on how they are using emerging tools to build and disseminate new knowledge. And for current policy analysts who want to deepen their skills, they should consider applying to the Computational Social Science Summer Institute, a free two-week intensive to learn data skills in the context of social problems and policy data. Though much of it is not directly relevant to policy, there is a tremendous amount of online content for self-learners, too. I recommend looking into free online courses and learning to program in R. For those interested in a bigger investment, look to the joint data science and public policy graduate programs, like those at Georgetown University, the University of Chicago, and the University of Pennsylvania.The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.Google provides general, unrestricted support to the Institution. The findings, interpretations, and conclusions in this report are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.",https://www.brookings.edu/research/what-all-policy-analysts-need-to-know-about-data-science/
data science,Bridging the gender data gap,"More men than women are killed in car crashes each year, partly because men drive more and engage in riskier driving behavior. On the other hand, women are 17% more likely to be killed and 47% more likely to be injured in crashes than men are. Women are at increased risk simply because they are women: cars are primarily designed, built, and tested by male engineers using male data, so they are built with men in mind. Scaled-down versions of male crash test dummies, meant to represent women, were not used until 2003—and are primarily tested in the passenger seat. In car design, development, and testing, male bodies are the standard and female bodies the outlier. This creates a gender data gap with very real impacts on the lives of Americans.JJeanette Gaudry HaynieFounder and Executive Director - Athena Leadership Project Lieutenant Colonel - U.S. Marine Corps Reserve The gender data gap appears in health and medicine, too. Heart disease kills more people in the United States than anything else, accounting for approximately 20% of female deaths and 25% of male deaths. But survival rates for women are substantially worse than they are for men, and the gender data gap is partly to blame. Historically, heart disease research was primarily conducted on male subjects by male scientists and doctors, so male symptoms are considered typical and female symptoms atypical. As a result, women are misdiagnosed up to 50% more often and are more likely to be dismissed without treatment. Even when treated, women are less likely to receive needed medications or advice. And simply including women in research is not enough: when research findings are not sex-disaggregated, men are still considered the norm.[1]The gender data gap affects military service as well. Retention of women is a perennial, persistent, and well-established problem, but the services have never meaningfully tracked why women leave the military at higher rates. A range of factors—rigid career paths, restrictive policies on parental leave and postpartum fitness, and cultural problems such as the Marines United scandal and high sexual assault rates—are suspected to disproportionately impact women. Yet the primarily-male leadership only recently adopted a recommendation to start tracking retention drivers on a voluntary basis. These efforts will remain inadequate until the scope of analysis is broadened, population pool is increased, and time has passed. Until then, the gap remains robust.Rethinking the defaultThe endangerment of American citizens is concerning enough, but the gender data gap also creates a vicious cycle of underrepresentation in leadership across the nation. When leaders are primarily male, they are less likely to consider women as they develop and implement policy. And the policies they enact further neglect women, so the cycle continues. When men become the default, women are an afterthought. Their bodies, needs, and contributions are neglected, and their participation as leaders at every level is limited.If the male perspective is assumed to be the standard and the female perspective is the “other”, the institutions that rest on that assumption, the decision-making processes leaders employ, and the policy choices that result suffer. This limits government effectiveness, and nowhere is this more critical than in the context of national security. To understand security challenges and develop means to successfully address them, leaders must seek the fullest picture possible. If women are not at the decision-making table and those in command do not consider female perspectives, that picture remains incomplete.Closing the gapFor this nation to succeed and prosper in the future, the gender data gap must be recognized and it must be narrowed.  There are three main steps to accomplish this.Related FixGovKeeping his promises? Black presidential appointments in the Biden administrationNicol Turner Lee and Kathryn Dunn TenpasFriday, December 4, 2020 Up FrontThe challenges facing Black men – and the case for actionRichard V. Reeves, Sarah Nzau, and Ember SmithThursday, November 19, 2020 Brown Center ChalkboardHow our education system undermines gender equityJoseph CimpianMonday, April 23, 2018First, research must explore the gender data gap. Instead of considering the average male the norm, gender diversity in research must be actively sought. By exploring how diverse people are affected by various phenomena, research findings can inform government structures, their functions, and the policies that result.Second, research must explore how diverse teams and leaders impact organizations, particularly in government. The business case for diversity is being established, and research into diversity in other areas is promising: research into diversity in medicine found that female doctors have higher patient survival rates, and male doctors’ performance improved when they worked with female physicians. But the rest of the country must catch up. Research can demonstrate how diversity, particularly gender diversity, affects American lives.Related Books Turning PointBy Darrell M. West and John R. Allen 2020 MarijuanaBy John Hudak 2020 Divided Politics, Divided NationBy Darrell M. West 2020Finally, leaders must actively seek diversity. While the gender data gap is narrowed through research, including the perspectives of those who live in the gap can fill in the blanks. There is precedent for this. United Nations Security Council Resolution 1325, adopted 19 years ago, established that women must be included at all levels of leadership to achieve peace and stability. The United States’ own Women, Peace, and Security Act of 2017 states that “the political participation and leadership of women…is critical to sustaining democratic institutions; and… helps promote more inclusive and democratic societies and is critical to country and regional stability.” For policies to work, institutions to function, and security to last, diverse perspectives must be included.Until the gender data gap is narrowed, the lack of data, and the underrepresentation of women that feeds and is fed by it, will continue to limit the effectiveness of private, public, and non-profit organizations. But the gender data gap will not close on its own—we must actively seek to close it, the sooner the better. As Malala Yousafzai declared in her 2013 address at the United Nations, “We cannot all succeed when half of us are held back.”[1] For full discussion of the extent of the gender data gap, see Invisible Women: Data Bias in a World Designed for Men by Caroline Criado Perez.",https://www.brookings.edu/blog/techtank/2019/11/20/bridging-the-gender-data-gap/
data science,"Big data, meet behavioral science","America’s community colleges offer the promise of a more affordable pathway to a bachelor’s degree. Students can pay substantially less for the first two years of college, transfer to a four-year college or university, and still earn their diploma in the same amount of time. At least in theory. Most community college students—80 percent of them—enter with the intention to transfer, but only 20 percent actually do so within five years of entering college. This divide represents a classic case of what behavioralists call an intention-action gap. Why would so many students who enter community colleges intending to transfer fail to actually do so? Put yourself in the shoes of a 20-something community college student. You’ve worked hard for the past couple years, earning credits and paying a lot less in tuition than you would have if you had enrolled immediately in a four-year college or university. But now you want to transfer, so that you can complete your bachelor’s degree. How do you figure out where to go? Ideally you’d probably like to find a college that would take most of your credits, where you’re likely to graduate from, and where the degree is going to count for something in the labor market. A college advisor could probably help you figure this out, but at many community colleges there are at least 1,000 other students assigned to your advisor, so you might have a hard time getting a quality meeting.  Some states have articulation agreements between two- and four-year institutions that guarantee admission for students who complete certain course sequences and perform at a high enough level. But these agreements are often dense and inaccessible. The combination of big data and behavioral insights has the potential to help students navigate these complex decisions and successfully follow through on their intentions. Big data analytic techniques allow us to identify concrete transfer pathways where students are positioned to succeed; behavioral insights ensure we communicate these options in a way that maximizes students’ engagement and responsiveness. The transfer decision is of course only one of many such challenges students and families face in education, where information to help people make informed choices is theoretically available, but often complex and poorly communicated. Parents of toddlers in many states can now access early childcare rating information through Quality Rating Improvement Systems, but in most locations these systems have low visibility. High school students in many schools have a range of courses available to them, but they may not get sufficient counseling on which course sequences best position them for postsecondary education. A growing body of innovative research has demonstrated that, by applying behavioral science insights to the way we communicate with students and families about the opportunities and resources available to them, we can help people navigate these complex decisions and experience better outcomes as a result. A combination of simplified information, reminders, and access to assistance have improved achievement and attainment up and down the education pipeline, nudging parents to practice early-literacy activities with their kids or check in with their high schoolers about missed assignments, and encouraging students to renew their financial aid for college. Big data analytic techniques have the potential to contribute to a next generation of behavioral science interventions in education—strategies that help people overcome complex matching problems like the transfer decision by giving personalized information about educational pathways that both build on the students’ prior experience and best position them for future success. Think of this as Netflix’s movie recommendation algorithm applied to education. There have already been some initial ventures in this direction. In one seminal study, researchers capitalized on student-level academic achievement data from the College Board and publicly-available data on college characteristics to send high-achieving, low-income students semi-customized packets of high-quality colleges and universities where students had a good chance of being admitted based on their academic performance. But there’s also real potential for much greater personalization. Take a community college student’s transfer decision as an example. Working with a state higher education system, researchers and data scientists could use a combination of articulation agreements and data from prior cohorts of students to identify optimal transfer pathways for different student groups and course-taking histories. Using data on current students’ course-taking experiences and contact information they provide to their college, the system could proactively send students messages that identifies specific institutions where they have already met the transfer requirements, or where they are on track to do so, and where prior cohorts of students from their same college have been successful upon transferring. Behavioral scientists can help design this outreach in a way that maximizes student engagement and responsiveness.  These types of big data techniques are already being used in some education sectors. For instance, a growing number of colleges use predictive analytics to identify struggling students who need additional assistance, so faculty and administrators can intervene before the student drops out. But frequently there is insufficient attention, once the results of these predictive analyses are in hand, about how to communicate the information in a way that is likely to lead to behavior change among students or educators. And much of the predictive analytics work has been on the side of plugging leaks in the pipeline (e.g. preventing drop-outs from higher education), rather than on the side of proactively sending students and families personalized information about educational and career pathways where they are likely to flourish. From pre-K through college, students and families have to navigate one complex decision after another—about which schools to attend, what courses to take, which programs to apply for. These decisions are particularly daunting for socioeconomically-disadvantaged families who have less experience navigating complicated educational decisions and less access to professional advising. Families like these also have to devote a substantial portion of their cognitive bandwidth to just making ends meet financially. By leveraging big data techniques and behavioral science insights, we can help families identify educational opportunities that position them for ongoing success. Related Books Turning PointBy Darrell M. West and John R. Allen 2020 MarijuanaBy John Hudak 2020 Divided Politics, Divided NationBy Darrell M. West 2020 Ben CastlemanAssociate Professor, Education and Public Policy - University of Virginia",https://www.brookings.edu/blog/brown-center-chalkboard/2016/03/10/big-data-meet-behavioral-science/
data science,Robot census: Gathering data to improve policymaking on new technologies,"There is understandable excitement about the impact that new technologies like artificial intelligence (AI) and robotics will have on our economy. In our everyday lives, we already see the benefits of these technologies: when we use our smartphones to navigate from one location to another using the fastest available route or when a predictive typing algorithm helps us finish a sentence in our email. At the same time, there are concerns about possible negative effects of these new technologies on labor. The Council of Economic Advisers of the past two Administrations have addressed these issues in the annual Economic Report of the President (ERP). For example, the 2016 ERP included a chapter on technology and innovation that linked robotics to productivity and growth, and the 2019 ERP included a chapter on artificial intelligence that discussed the uneven effects of technological change. Both these chapters used data at highly aggregated levels, in part because that is the data that is available. As I’ve noted elsewhere, AI and robots are everywhere, except, as it turns out, in the data.Robert SeamansAssociate Professor and Director of the Center for the Future of Management - Stern School of Business, NYU  TwitterRobSeamansTo date, there have been no large scale, systematic studies in the U.S. on how robots and AI affect productivity and labor in individual firms or establishments (a firm could own one or more establishments, which for example could be a plant in a manufacturing setting or a storefront in a retail setting). This is because the data are scarce. Academic researchers interested in the effects of AI and robotics on economic outcomes have mostly used aggregate country and industry-level data. Very recently, some have studied these issues at the firm level using data on robot imports to France, Spain, and other countries. I review a few of these academic papers in both categories below, which provide early findings on the nuanced role these new technologies have on labor. Thanks to some excellent work being done by the U.S. Census Bureau, however, we may soon have more data to work with. This includes new questions on robot purchases in the Annual Survey of Manufacturers and Annual Capital Expenditures Survey and new questions on other technologies including cloud computing and machine learning in the Annual Business Survey.While these new data are a promising step, there is still a need for a large-scale survey of technology use across multiple sectors of the economy. Congress should fund the U.S. Census Bureau to collect this data. The work that Census has done so far—for example by collecting data on the purchase and use of robotics in the manufacturing sector, via its Annual Survey of Manufacturing—provides a blueprint for how this can be done across other sectors of the economy. With better data, researchers will be able to measure the effects of these technologies on a range of issues including productivity, employment, training, inequality and regional competitiveness, and policy makers will be able to develop well-informed policy—or tweak, update, or eliminate existing policy.Robots create and destroy jobs in manufacturingMost studies of how robots affect the economy have used data published by the International Federation of Robotics (IFR), a trade association that collects data from its members. For example, Georg Graetz and Guy Michaels used the IFR data for 17 countries for the period 1993 to 2007 to show a positive link between robots and productivity. Daron Acemoglu and Pascual Restrepo used IFR data to study the effect of robot exposure on U.S. manufacturing jobs. They found that one robot per thousand manufacturing workers reduces the employment-to-population ratio by about 0.18-0.34 percentage points.Related Content Up FrontOpportunity, policy, and the future of automationMarcus Casey and Ashleigh MaciolekMonday, December 21, 2020 Op-EdNot all robots take your job, some become your co-workerAaron KleinWednesday, October 30, 2019More recently, several studies have used data on robot imports to study the effect of robots on employment outcomes at firms. Using data from several French government sources, Acemoglu, Claire Lelarge, and Restrepo found that, in French manufacturing firms, those firms that adopted robots added jobs. This finding, which runs counter to the popular notion that “robots are coming for our jobs,” reflects the same positive relationship between robot adoption and jobs documented by researchers in other countries, including Canada, Denmark, and Spain. In other words, robots may be good for employment, at least at adopting firms in advanced economies. There is one big gap in the literature, however—we don’t yet have the data needed to do a similar study in the U.S.Acemoglu, Lelarge, and Restrepo also found that manufacturing firms are likely to lose jobs when their competitors adopt robots. Moreover, they found that, on net, the negative effects on employment at other firms dominate the positive effects at robot adopting firms: even as some manufacturing firms grow and add jobs (those adopting robots), a larger number of manufacturing firms shrink and lose jobs. This same result has also been found in a study by Koch, Manuylov, and Smolka using data from Spain. Again, due to lack of data, we don’t know if the same effect occurs in the U.S.These and other recent studies make it clear that the relationship between robots and jobs is nuanced, at least in manufacturing settings in advanced economies. There are still a number of outstanding questions about the relationship between robots and jobs, as there are for AI and other new technologies:Why don’t all firms adopt robots if they can, especially since those that don’t adopt robots seem to suffer employment losses? Does the relationship between robots and firm-level employment also hold in the case of other technologies, like AI?What happens to workers who lose their jobs at firms that don’t adopt robots? Do they end up working at other firms that adopt robots? Is the same true for workers at firms that don’t adopt AI?When firms that adopt robots add jobs, what types of workers do they hire, and are they well paid? What about the skills and wages for workers at firms that adopt AI?More high-quality data from government statistical agencies will help researchers address these questions.Recent U.S. Census Bureau measurement effortsIn addition to its important work surveying the population every ten years, the U.S. Census Bureau routinely surveys business establishments and firms about a range of issues, including revenues, expenses, wages, and others. The data collected from these surveys help government agencies to estimate GDP, employment, wage growth, trade deficits, and other factors to predict how current macroeconomic conditions and government policies are affecting the economy, workers, and households.The U.S. Census Bureau has started measuring the use of robotics in U.S. establishments and firms through the Annual Survey of Manufactures (ASM) and the Annual Capital Expenditures Survey (ACES). It also measures the use of AI, cloud hosting services, robotics, and other technologies in U.S. firms through the Annual Business Survey (ABS). A recent video conference jointly hosted by New York University and the U.S. Census Bureau highlighted some of the early findings from these surveys and sought feedback for soon-to-be-released experimental data products from experts in the field.In 2018, the ASM, an annual sample survey of approximately 50,000 manufacturing establishments, included three robotics-related questions. The survey asked about capital expenditures on robots, the number of new robots in 2018, and the total stock of robots in 2018. Funding for the cognitive testing of these questions—a necessary step to ensure that respondents understand the questions being asked—was provided by the National Science Foundation. Initial evidence from the survey indicates that manufacturing establishments that adopt robots tend to be larger (as measured by number of employees). Robots are used in most manufacturing industries across many U.S. states, but the states with the largest percent of manufacturing establishments using robots are in the industrial Midwest. Preliminary estimates show robot exposure rates—the share of workers working next to robots—exceed 30 percent in the Transportation Equipment, Primary Metal, and Plastic and Rubber Products industries.The ACES, which surveys approximately 50,000 firms in a variety of industrial sectors about their capital expenditures, included a single question on robotics expenditures in its 2018 survey. The question mirrored the capital expenditure question asked of establishments in the 2018 ASM survey, but at the firm level (a firm can have multiple establishments). This survey similarly found that firms adopting robots tend to be larger (as measured by number of employees). The manufacturing sector had the highest total capital expenditures on robots and highest average by firm. Other industries with high capital expenditures on robots include non-store retailers and hospitals. The ACES is the only survey instrument that delivers data on capital expenditures in the U.S. from a representative sample of firms across all economic sectors.In 2018 the ABS included a number of questions about technologies used at the firm. It asked whether firms use cloud-based services such as servers, data storage, data analysis, and customer relationship management, and business technologies such as machine learning, machine vision, touchscreens, and robotics. The big takeaways from this survey are that digitization has been widely adopted by all firms and sectors; diffusion is highest among the oldest and largest firms; and technology usage increases with size in all age categories. Cloud-based services have been less widely adopted but are used for many different functions. There is high variability in type and use by sector: manufacturing is a leading adopter of certain technologies, such as machine learning, machine vision, and robotics. The ABS also finds ample evidence of complementarities between technologies: advanced technology adoption is highly dependent on the adoption of key infrastructure. More detail about findings from the ABS are available in a recently released NBER publication.It is important for the U.S. government to conduct more systematic data collection on the use of robotics and other new technologies in our economy. At a minimum, government data can be used to replicate the existing robot studies that rely on the IFR data. But the disaggregated firm-level and establishment-level data can also help us understand the conditions under which robots complement or substitute for labor and help policymakers design and evaluate the appropriate policy responses. Moreover, government data could help us understand whether the effects that are emerging in the case of robotics also hold for AI and other technologies.More funding for more measurement While the recent efforts of the U.S. Census Bureau are an important first step, there is more that could be done if the funding were available. In the late 1980s, the Census Bureau conducted the Survey of Manufacturing Technology (SMT). The purpose of the SMT was to measure the presence, use, and planned use of advanced technologies in the manufacturing sector. The Survey was administered in years 1988, 1991 and 1993 but was discontinued for funding reasons. Congress should provide funding to Census to conduct a modern, standalone version of the SMT. Ideally this new survey would be a short, annual, standalone survey of technology use at the establishment level across multiple industries in the economy. The survey would include questions about the use of specific technologies, such as robots, machine learning, cloud, e-commerce, autonomous guided vehicles, and others, and could be a simple “yes/no” question about whether the establishment has the technology or not. Questions about new technologies could be added in the future. It is important for the survey to be annual, so that changes in technology use could be tracked over time. An establishment level survey would allow for a granular analysis of adoption of a specific technology at that establishment on workers at that same establishment. In contrast, data that comes from firm level surveys make it harder to establish a causal link between adoption of a technology and effects on workers because firm level surveys aggregate information from all the establishments owned by the firm. In addition, since establishments are linked to a specific geography, an establishment level survey would allow for an analysis of how new technologies affect employment, inequality and other outcomes in different localities. The data could also be used to benchmark U.S. technological adoption relative to other countries.Ideally, Congress would realize the value of such a survey and fund the Census Bureau to create it. The biggest challenge for such a survey is cost. There are two types of costs: the upfront costs of creating a new survey, which would primarily be the cost of conducting cognitive testing of the survey questions, and the recurring costs of administering the survey annually. These costs are hard to estimate and depend on the number of questions asked and number of establishments surveyed. The Census Bureau’s prior experience working with external researchers on the Management and Organizational Practices Survey (MOPS), which involved the creation of a standalone survey, may provide a useful benchmark on costs. The costs of developing and administering the MOPS survey was partially defrayed by use of grant funds from the National Science Foundation (NSF) from external researchers. This was also the case with the development of the robotics questions for the ASM, which benefited from a NSF grant. A similar model could be used to help defray some of the costs of a standalone technology survey.There would be a variety of factors for the Census Bureau to consider when designing a new survey. The Census Bureau’s experience developing questions on the establishment level purchase and use of robots for the ASM should be useful. The cognitive testing of those questions, which is documented in Buffington, Miranda, and Seamans (2018), involved in-person interviews with plant managers to assess their understanding of the question and their ability to access the data necessary to accurately answer the questions about number of robots and capital expenditure on robots. The Census Bureau would need to do similar cognitive testing of all the questions in any new standalone survey. On one hand, the testing would be more involved than what was done for robot questions in the ASM as it would involve assessing the ability of managers across multiple sectors of the economy to answer the questions. On the other hand, the testing may be easier as it would involve a single question for each technology—either the establishment has it or not—rather than requiring an estimate of capital expenditures on those technologies, as was done in the ASM.There would be multiple benefits to a standalone survey of technology. The survey would allow researchers to identify sectors and regions of the economy that are being impacted by new technologies. When linked with other data sets, researchers would be able to assess the effects of these technologies on workers and firm level outcomes such as productivity, growth, or firm exit. For example, the data could be linked with establishment-level data from the Annual Survey of Manufacturers to study the effect of these technologies on establishment productivity. Or the data could be linked to firm level occupational data—such as the micro-data from the Bureau of Labor Statistics (BLS) Occupational Employment Survey, which is confidential but available to BLS-approved researchers—to identify effects of technologies on workers by occupation. An additional benefit of such a survey is that it may help the BLS improve measurement of multifactor productivity—a measure of how efficiently our economy transforms inputs, including labor, capital, technologies and know-how, into outputs. Accurate productivity statistics help the government assess the overall well-being of the economy and decide when fiscal or monetary policy action needs to be taken to address slowing growth. Some have argued that multifactor productivity suffers from mismeasurement, which may stem in part from not being able to account for the role of new technologies. See Byrne, Fernald, and Reinsdorf (2016) for a useful review of the potential role of mismeasurement.In summary, while there is excitement about the impact that new technologies like artificial intelligence and robotics will have on our economy, we need to do more to measure where and how these technologies are being used. A good place to start would be additional funding from Congress to the U.S. Census Bureau to conduct an annual standalone survey of technology use across establishments in the U.S. economy—in short, it’s time for a Robot Census.The author did not receive financial support from any firm or person for this article or from any firm or person with a financial or political interest in this article. He is currently not an officer, director, or board member of any organization with an interest in this article.",https://www.brookings.edu/research/robot-census-gathering-data-to-improve-policymaking-on-new-technologies/
data science,Platform data access is a lynchpin of the EU’s Digital Services Act,"The European Commission has put forth a first draft of the Digital Services Act (DSA), which aims to create a consistent set of rules for internet companies across the European single market. Tucked into this proposal is a requirement which would enable academic researchers to access data from the largest internet platforms. Modest as it may sound, this subtle provision is key to the success of this legislation.Alex EnglerRubenstein Fellow - Governance Studies  Twitter@AlexCEnglerWhile some of the DSA’s new rules cover internet access providers, most are concerned with internet services that host content. For instance, all hosting services—including cloud providers, web hosting services, and online platforms—would be required to implement a “notice and action” process for users to inform the service of illegal content, to which that service is then required to respond. Excluding the smallest companies,[1] online platforms such as social media and marketplaces get the most compliance requirements. Online platforms would need to enable complaints about their illegal content decisions, suspend service to users who repeatedly post illegal content, and be more transparent about content moderation and targeted advertising. This proposed legislation goes even further for especially large online platforms, creating a new oversight scheme for platforms with over 10 percent of the EU’s population in active monthly users. This is likely to include Facebook, Twitter, YouTube, Amazon, and TikTok, as well as potentially Instagram, PornHub, and Google Maps.[2]This oversight scheme is complicated, requiring several additional layers of transparency. First, there are a range of public-facing transparency requirements, including an annual report on content moderation efforts, labels that tell users why they are seeing targeted advertisements, a public database of targeted advertisements, and more transparency on how their recommender systems work. Second, the large platforms must conduct a self-assessment of systemic risks, which they are then required to work to mitigate (or risk large fines) under the proposed law. Third, the platforms must pay for an independent audit of compliance with the holistic requirements of the legislation. All of these transparency requirements are potentially meaningful, but it is the fourth and last mechanism that offers the most promise: the largest internet platforms must open up their data to independent researchers approved by the European Commission.Article 31 of the DSA states that large internet companies need to comply with data requests from researchers once each request was approved by the EU country which hosts that technology company, or the European Commission. To reduce the risk of privacy breaches and corporate espionage, only vetted researchers with affiliations to academic institutions and relevant expertise will be granted data access, notably not including journalists and activists. Further, the researchers would not be allowed to use the data for profit-seeking purposes, or to inform political campaigns, as was the case in the Cambridge Analytica scandal. Researchers could make these requests only to conduct research related to illegal content (e.g., child pornography, terrorist content, hate speech), manipulative use of the platforms (e.g., disinformation campaigns), and a broader category of negative effects including discrimination and child protection (e.g., self-harm and suicidal behavior resulting from cyberbullying).This is an enormously important provision, because it harnesses the capability and motivating incentives of researchers to examine and challenge the decisions of the big internet platforms. It will help to close the information gap between lawmakers and technology companies, which is perhaps the most universal problem in technology policy.Researcher data access is critical to ensuring compliance Oversight of the large online platforms is complicated for many reasons, not least of which because policy makers and the public have so little insight into the comprehensive workings of these social systems. Despite the widespread impression of far right-wing news dominating Facebook, it’s actually impossible to know if that’s the case with currently available data. While Google produces a wide range of academic research, the recent dismissal of Dr. Timnit Gebru suggests those papers do not provide an unvarnished view of the search giant. The industry-academic partnership Social Science One, an ambitious effort funded by Facebook, strived to balance user privacy and researcher access, but ultimately was not able to offer researchers sufficiently complete data to answer the most pressing questions. Social Science One’s entire European advisory committee stepped down in December, saying the project did not fulfill its goals. The voluntary measures taken by the internet platforms to enable researcher access are simply not working. In perhaps the worst case, Uber has strategically allowed access to datasets to spread a favorable corporate narrative.Related Media & JournalismHow to combat fake news and disinformationDarrell M. WestMonday, December 18, 2017 PrivacyWhy protecting privacy is a losing game today—and how to change the gameCameron F. KerryThursday, July 12, 2018 TechTankCOVID-19 has taught us the internet is critical and needs public interest oversightTom WheelerWednesday, April 29, 2020These circumstances explain why the DSA includes a provision on data access for independent researchers—there is no other insight into large online platforms, so this research is necessary to better inform the public and the various governments of the EU. It would offer an unvarnished understanding of the scale and spread of illegal and harmful content online, from health misinformation to the sale of counterfeit goods.Further, the researchers would be able to examine the choices made by the technology companies and better consider the alternatives they eschewed. In 2019, YouTube has claimed that it has made algorithmic changes leading to its users watching fewer fringe videos that might misinform or even radicalize them. More recently, Instagram promised to use algorithms to reduce unlabeled advertising by its influencers. Yet there is no way to verify these claims or consider if there were alternative interventions that would have been effective. Data access for independent researchers would change this, shedding light both on the state of the online world and the decisions that online platforms were making to shape it. In turn, this enables sharper criticisms of the platforms. A more precise conversation about what interventions they should take could lead to more impactful changes.Independent researchers would also provide a genuinely independent check on the platform’s self-assessment and the independent audit required by the DSA—likely making these more accurate and revealing. The platform’s self-assessment cannot be relied upon alone if one expects this legislation to change platform behavior. The DSA also requires that the large platforms pay for an independent audit, which is potentially more impactful. However, there will undoubtedly be a large financial incentive for those auditors, chosen by the platform companies, to be amenable to their client’s perspective. The credible possibility that an independent academic study, with the same level of data access, will publicly refute the platforms assessments would go a long way to keeping both reports honest.Related Books Turning PointBy Darrell M. West and John R. Allen 2020 MarijuanaBy John Hudak 2020 Divided Politics, Divided NationBy Darrell M. West 2020The DSA also empowers a group of trusted flaggers, who will also benefit from the researcher data access provision. Under the DSA, trusted flaggers are entities to be approved by the EU as having expertise and competence in identifying illegal content. Internet platforms would have to prioritize responses to illegal content notices submitted by these trusted flaggers. Since they would be required to represent the “collective interests,” many trusted flaggers would be non-profit groups and journalists looking to improve the online ecosystem. Notably, the DSA requires that platforms be able to accept illegal content notices in bulk, enabling trusted flaggers to submit many illegal content notices at once. The presents an opportunity for independent researchers to use their privileged access to platform data to help guide the trusted flaggers. By finding patterns in illegal content dissemination and building tools for their detection, academic researchers can help increase the amount of illegal content reported, with expedited responses from the large platforms. In some cases, the research groups might try to become trusted flaggers themselves. This might be especially impactful because the researchers can report content not visible to the general public.Better information directly impacts other compliance mechanisms in the DSA, too. It will enable the European Board for Digital Services to accurately and fairly levy fines, which can be as high as six percent of annual revenue for the companies. For instance, researchers can examine whether the internet platforms are appropriately suspending accounts that are frequently posting manifestly illegal content, as would be required by the DSA. In short, the independent researcher data access provision of Article 31 strengthens every other oversight and compliance provision within the DSA, and could be the lynchpin of the legislation’s efficacy.In fact, it is important enough to the success of the legislation that it presents a potential point of failure. The researchers are reliant on the large platforms to honestly provide datasets without edits or omissions, but will not often have the means to evaluate whether this is the case. The platforms are not likely to outright mislead the regulators though, as the Volkswagen emissions scandal demonstrated, this possibility should not be entirely discounted. Still, many of the platforms’ decisions at the margins will affect the usability of the data as well as the picture that data paints. In order to make sure this compliance system work, the Commission should ensure that the required independent audit confirms that platforms are honestly and completely fulfilling the researcher data access requests.The specifics of the implementation will matterCurrently and without the DSA provisions, while many researchers are greatly interested in analyzing internet platforms companies, they are restrained in how they can do this by technical, legal, and statistical challenges. For instance, much of the data from Facebook is private, and even for the public websites, the outside view is restricted. While a researcher can see what tweets are liked or shared, they cannot know what tweets a user saw, but did not interact with. In other fields, taking a random sample can be effective way to study large-scale problems, but the important questions of the web are dependent on the networks—the connections between users—and sampling from networks is exceptionally challenging. As the statistician Andrew Gelman writes, “The fundamental difficulty of network sampling is that a small sample of a network doesn’t look like a network itself.” Even though Twitter and YouTube offer some data access through APIs, it is not nearly enough to sufficiently understand how their networks work. By providing researchers with expanded data access from the internet platforms, the DSA would resolve many of these issues.Of course, with this change would come new implementation challenges. The current draft suggests that the large platforms need to provide researchers with the data, and those researchers must commit to preserving data security and confidentiality requirements that will be specific to each data access request. While many academic researchers may have experience working with private and sensitive data, these datasets may pose especially high risks. It could include Facebook posts meant only for family members, YouTube videos watched in private, and even possibly searches on PornHub. This data can be kept anonymized to the researchers, as the academic research community has strong safeguards and practices for maintaining confidentiality. However, some of these datasets will be of interest to outside actors for enabling blackmail and identity theft, or stealing trade secrets. It is not clear that most academic researchers will have data privacy and security standards that are robust to skilled and motivated hackers.The platforms will be understandably more hesitant to hand over large sensitive datasets to many different researchers, even without legal liability for disclosures. More specific data is much more valuable to researchers and will lead to more impactful research, but it’s also inherently a great privacy risk, since it contains more information to identify individuals. Platforms will likely be concerned that even if they aren’t at fault for a data breach, they still may be blamed by their users.The European Commission is right to incorporate researcher data access into the DSA, but it may need to play a larger role in enabling this access. As an alternative to putting the security onus on researchers, the Commission could set up a centralized process that enables secure data access to researchers without this capacity. The United States has effectively implemented this through systems like the Census Bureau’s Statistical Research Data Centers and the Coleridge Initiative’s Administrative Data Research Facility. The Secure Data Access Center in France works similarly, enabling researcher access to information such as sensitive health data. Another approach would be for the Commission to provide grant funding for a small number of trusted third-party facilities to set up this capacity for researchers. This would also help alleviate the cost of analyzing these massive datasets, by preempting the need for each research group to separately create secure analytical environments.The Commission has correctly identified how important researcher data access will be to the effective oversight of large companies. In a sense, the European Commission is outsourcing enforcement and oversight to independent researchers, rather than creating a new agency. If implemented carefully, this facet of the Digital Services Act will be key to its success, contributing to a better public debate and more responsible actions by large internet platforms. If this measure works effectively, expanding the number of companies covered will be worth considering. Even without expanding the rest of the compliance requirements, better researcher access to platform datasets is a worthwhile investment. Legislators in the United States should also pay attention, as the proposals put forth by the European Commission in the Digital Services Act may be models for responsible internet governance.[1] The online platform requirements do not apply to micro or small businesses, and so only companies with more than 50 staff or more than €10 million in annual turnover need to comply.[2] Messaging applications, such as WeChat and WhatsApp, are not included. Further, while Netflix and Spotify might qualify as very large online platforms, this is unlikely to significantly affect them since they allow such limited contributions from their users.Amazon, Facebook, and Google are general, unrestricted donors to the Brookings Institution. The findings, interpretations and conclusions in this piece are solely those of the author and not influenced by any donation.",https://www.brookings.edu/blog/techtank/2021/01/15/platform-data-access-is-a-lynchpin-of-the-eus-digital-services-act/
data science,Asia’s data frontier—Modeling poverty from space,"As we celebrate World Statistics Day, data is impacting our lives more and more, not only in how we manage our private lives but increasingly in how governments operate. A growing number of countries, especially in Asia, are using data to design policies, manage operations, and respond to crises. It has become the lifeblood of evidence-based policymaking. Given this pivotal role, having accurate, high-quality, trustworthy data is more crucial than ever. The unprecedented COVID-19 pandemic underscores the importance of getting credible and reliable data. It is an essential tool for designing responses, allocating resources, and measuring effectiveness of interventions.Anna Marie FernandoEconomics and Statistics Specialist / Consultant - Asian Development Bank Arturo Martinez Jr.Statistician - Asian Development Bank Joseph BulanAssociate Statistics Analyst - Asian Development Bank Katharina FenzLead Data Scientist - World Data Lab According to World Data Lab’s World Poverty Clock, global poverty by the end of 2020 could increase to of the world’s population. In fact, the pandemic has reversed recent decreases in poverty. Instead of reducing poverty by 20 million people, the pandemic is pushing at least another 60 million people into extreme poverty. Hence, special attention must be given to the poor as they are the most vulnerable to the adverse socioeconomic impact of COVID-19.Granular poverty statistics are paramount to facilitate more efficient, targeted interventions to address the various needs of the different segments of the poor. However, conventional sources of poverty data, such as surveys of household income and expenditures, do not provide sufficient granular data, nor are they timely enough to be useful for policymakers. We need to look for alternative sources to supplement and integrate with traditional sources.Digital technology has created a tidal wave of data, which the Asian Development Bank (ADB), together with its partners, is starting to exploit for the design of development projects. These innovative data sources can potentially address the limitations of traditional sources of poverty data, such as the issue of granularity. Inspired by a method proposed by Stanford University researchers, researchers from the ADB examined the feasibility of integrating household surveys and satellite imagery, and used machine learning algorithms to predict the spatial distribution of poverty.Research results are now available for a joint project between officials in the Philippines, Thailand, and the World Data Lab. These two countries were chosen for their existing initiatives to combine household survey data with census data to produce more granular yet reliable poverty estimates. These initiatives provided adequate data to train machine-learning algorithms. Additionally, the Philippines and Thailand have slightly different poverty profiles, with the latter showing significantly lower poverty rates (Figure 1). This allowed us to examine how poverty distribution might affect granular poverty estimation that incorporates data from satellite imagery.The research team trained the algorithm to predict the intensity of night lights based on daytime satellite imagery, as studies show that nighttime light intensity is a reasonable proxy for the level of economic activity. The goal was to get a model that can recognize features in daytime satellite imagery—such as street patterns, building density, and roofs—that lead to high levels of luminosity at night. The researchers then used aggregates of those features to analyze their relationship with poverty rates at those levels, which represent the most granular level of our input data. When a relationship was established, it was possible to predict the prevalence of poverty for an area as small as 4 kilometers by 4 kilometers.Figure 1. Modeling Thailand’s poverty from spaceSource: ADB. Mapping Poverty through Data Integration and Artificial Intelligence; September 2020The results are encouraging. After calibration, the predicted poverty estimates, produced by the machine-learning algorithms applied on publicly accessible satellite imagery, are not only aligned with government published estimates, but were also more granular. Better still, further gains in the granularity of statistics may be achieved if higher resolution imagery, such as those commercially sourced, is used in future studies.Studies such as this are part of development organizations’ efforts to strengthen national statistical systems to meet increasing data demands for effective policymaking and monitoring of development goals and targets. However, it is not enough to build the capacity of data compilers to integrate traditional sources with innovative technology. Nor is it enough to have trustworthy and reliable data. It is also important to ensure that data is used appropriately and communicated effectively.The COVID-19 pandemic may have turned the poverty clock back closer to levels we had when the SDGs were first launched five years ago. Now we have 10 years until the SDG reckoning. Accurate, timely, trustworthy, and granular data can help us move forward and traverse the path of socioeconomic recovery, as we draw closer to our goal of eradicating poverty and ensuring that nobody will be left behind.Related Content Future DevelopmentLearning in East Africa: Where are children advancingKatharina Fenz, Kristofer Hamel, and Baldwin TongThursday, May 28, 2020 Future DevelopmentUsing big data to assess London’s vulnerabilities and vitalityKatharina Fenz, Kristofer Hamel, Baldwin Tong, and Andreas WalliWednesday, April 8, 2020Note: This blog builds on an ADB report Mapping Poverty through Data Integration and Artificial Intelligence: A Special Supplement of the Key Indicators for Asia and the Pacific, developed jointly with World Data Lab. Its results were presented at a global webinar on Harnessing Data in the Digital Age for Poverty Reduction on October 14, 2020.",https://www.brookings.edu/blog/future-development/2020/10/20/asias-data-frontier-modeling-poverty-from-space/
data science,Charts of the week: Advancing women and girls in science,"“On this International Day, I urge commitment to end bias, greater investments in science, technology, engineering and math education for all women and girls as well as opportunities for their careers and longer-term professional advancement so that all can benefit from their ground-breaking future contributions.” — UN Secretary-General António GuterresThree years ago, the UN proclaimed February 11 the International Day of Women and Girls in Science. This new designation was part of a larger effort toward closing gender gaps around the globe, as outline in the 2030 Sustainable Development Goals. Though more women are pursuing careers in science, technology, engineering, and mathematics (STEM), it is clear that gender gaps in these fields—and harmful biases– persist today.Highlighted below are charts and commentary from Brookings experts on the state of gender equity in STEM fields, and the obstacles that women and girls still face.U.S. WOMEN EARN MORE COLLEGE DEGREES THAN MEN OVERALL, BUT EARN A MINORITY OF UNDERGRADUATE DEGREES ISSUED IN STEM FIELDSIn their study of gender disparities in education and employment, Ana Maria Munoz-Boudet and Ana Revenga, two experts from the World Bank, found that gender gaps in STEM fields are common around the world. According to the authors, in 2013 only four countries in Europe produced a pool of STEM graduates that were at least 15 percent female. In the United States, despite women earning more degrees than men overall, they account for only 35 percent of the undergraduate degrees issued in STEM fields.Munoz-Boudet and Revenga also note that in some areas, gender gaps in STEM fields are actually broadening. Between 2004 and 2014, the proportion of women earning engineering or computer science degrees in the United States fell.STEM FIELD FACULTY REMAINS PREDOMINANTLY MALEGender gaps in academia are also apparent at the faculty level. In a post for the Brown Center on Education Policy, University of Missouri Professor Cory Koedel and Diyi Li examined data from over 40 public universities to explore diversity and wage gaps among the faculty. They found that women only account for 18.1-31.1 percent of faculty in STEM fields, but as much as 47.1-53.2 percent of faculty in non-STEM fields.More research from the Brown Center illustrates how gender harassment and hostility in academia are keeping women from ascending the ranks in fields like economics.WOMEN ARE UNDERREPRESENTED THROUGHOUT THE INNOVATION PIPELINERecognizing that woman are still underrepresented in STEM fields, experts from the Hamilton Project at Brookings explored what effect that has on patenting and innovation. Women earn  57 percent of all four-year degrees, but only 35 percent of STEM bachelor’s degrees. Following degree completion, they account for just 22 percent of the STEM workforce, and are responsible for only 16 percent of granted patents.WOMEN REMAIN UNDERREPRESENTED IN THE MOST COMMON DIGITAL AND TECH JOBSFinally, in their study of occupational data and digitalization in the American workforce, experts from the Metropolitan Policy Program found a “mixed and sometimes surprising view” of male and female workers’ digital skills and employment. Data show that women are now slightly ahead of men as a whole when it comes to developing the digital skills increasingly essential for employment, but remain grossly underrepresented in some of the most common tech jobs such as computer programming and information systems management.Chris McKennaOffice of Communications",https://www.brookings.edu/blog/brookings-now/2018/02/09/charts-of-the-week-advancing-women-and-girls-in-science/
data science,Experts discuss the growth of cyber threats amid the pandemic,"On October 1, the Center for East Asia Policy Studies at Brookings held a roundtable discussion as part of its Asia Transnational Threats Forum series. The conversation focused on cybersecurity and cyber resilience in the age of COVID-19. Previous events honed in on cybersecurity, counterterrorism, and climate security. In line with this year’s objective to explore the impact of digital technologies on Asia’s security, economy, and political dynamics, the roundtable examined how technological tools have enabled social connections and successful management of the pandemic. The experts also explored how growing reliance on digital platforms has opened opportunities for increased cyberattacks and raised questions about data collection and privacy.EEun DuBoisResearch Assistant - Foreign Policy, Center for East Asia Policy Studies, The Brookings Institution Jung H. Pak, Brookings senior fellow and the SK-Korea Foundation Chair in Korea Studies, opened the discussion by noting an increased reliance on digital technologies amid the pandemic. She pointed to recent reports about upticks in cyberattacks around the globe, as more people on digital platforms increased the arena for cyber actors to exploit. She also observed growing concerns over countries’ use of technology to restrict freedom, using the pandemic as an opportunity to expand their control over their populations.David Koh, the commissioner of cybersecurity and chief executive at the Cyber Security Agency of Singapore, shared Singapore’s perspective in dealing with COVID-19 in the context of cybersecurity. He listed three major trends introduced by the pandemic: 1) an acceleration in digitalization and increased exposure of assets and infrastructure to cyberattacks; 2) an increase in collective risk profiles due to an expanded teleworking environment; and 3) a global surge in malicious cyber activities.With a wider spread of cyber incidents, protecting assets and infrastructure has become more challenging. As our societies return to normalcy, organizations must recalibrate their long-term cybersecurity posture in terms of risk management, Koh advised. To increase awareness on cybersecurity risks and precautionary measures, the Singaporean government has issued advisories to businesses and the public. He remarked that “like the coronavirus, cyber threats are borderless and asymmetric,” and highlighted the importance of a rules-based multilateral system to manage cybersecurity — the global commons.Related Books UpcomingNarco NoirBy Vanda Felbab-Brown 2025 UpcomingIran ReconsideredBy Suzanne Maloney 2025 UpcomingTo Rule the Waves: How Control of the World’s Oceans Determines the Fate of the SuperpowersBy Bruce Jones 2021Hiroaki Miyata, professor of health policy and management at Keio University’s School of Medicine, discussed the use of technology in public health policy and the implications of data sharing. Absent legal bases to impose a lockdown or use GPS technology for contact tracing, Japan collaborated with social networking services to identify, screen, and follow up on high-risk groups and patients. Miyata explained that using a large collection of data, the authorities were able to identify that retail and food service sectors were more vulnerable to COVID-19 compared to those engaged in stay-at-home jobs, thereby informing the follow-up practices to better control the virus’ spread.Transitioning to the issue of big data, Miyata argued that the value of data is in its sharing. Unlike oil, in which the exclusive right to use it is essential, he explained that the value of data increases with more data points in the system. He advocated for embracing the concept of data as a shared good and called for new rules around managing and sharing data.Anna Puglisi, senior fellow at Georgetown University’s Center for Security and Emerging Technology and former national counterintelligence officer for East Asia, argued that China has succeeded in quieting the scientific community’s questions about the origin of the virus while advancing the narrative that China is pro-science. At the same time, she said, China leveraged social media to promote its response to the virus and also repress dissent. These actions reflect China’s growing technical prowess and willingness to challenge global norms. While China’s science and technology ambitions are well known, its plans, policies, and goals can be abstract, Puglisi observed. However, COVID-19 has brought to light its role in the global science and technology community, and how China is willing to leverage it.Moving forward in a post-COVID-19 world, Puglisi recommended that the foreign policy community re-evaluate the global norm of collaboration in public health, which is built on trust. Will the experience with COVID-19 raise further questions about U.S. leadership in science? Another issue to consider, Puglisi concluded, is the impact of the data and information that is not freely shared. Will it hinder international institutions in battling future global crises?Related Content Order from ChaosBuilding resilience to the North Korean cyber threat: Experts discussEun DuBoisWednesday, December 23, 20202020Oct29Past EventAsia Transnational Threats Forum: Cybersecurity and cyber resilience9:00 AM -10:15 AM EDTOnline Only2019Dec16Past EventAsia Transnational Threats Forum: Climate change in Asia1:30 PM -4:45 PM ESTWashington, DC",https://www.brookings.edu/blog/order-from-chaos/2020/12/28/experts-discuss-the-growth-of-cyber-threats-amid-the-pandemic/
data science,Data and the transformation of international trade,"Globalization has entered a new phase, led by the increasing digitalization of international trade. Data flows are the conveyer belt of this transformation. New avenues for trade, innovation, and productivity growth are being created, but there are also risks. In my chapter in the just-published book “Growth in a Time of Change,” I examine the new opportunities and policy challenges.Access and use of data can be a driver of trade, innovation, and productivity growthData are driving innovation, with opportunity for a new wave of productivity growth. Currently, approximately half of the world is online. Between 2005 and 2021, global internet traffic will increase 127fold.  The rollout of the 5G network will support the ”internet of things” (IoT), such that by 2021, the number of devices connected to the internet will be triple the global population.Joshua P. MeltzerSenior Fellow - Global Economy and Development  Twitter@JoshuaPMeltzerThe ability to move data seamlessly and globally is supporting new business models, spurring research and development, facilitating international collaboration, and transforming international trade. Already, around 12 percent of global goods trade is via international e-commerce. E-commerce provides a potentially significant opportunity to increase small businesses’ participation in international trade, as having a website gives them an instant international presence, and e-commerce platforms provide embedded services such as financial payments and support with logistics. In Korea for instance, 100 percent of firms on eBay have cross-border sales, compared with 20 percent of offline firms.Services are being increasingly traded online. This includes information technology (IT), professional, financial, retail, and education services. New digital services, such as cloud computing, have been developed and are becoming crucial business inputs.Digital technologies are also being exported as part of traditional goods exports. For example, data collected from sensors on mining and farming equipment allow businesses to improve their operations and thereby the value from the use of such equipment.Global data flows underpin global value chains (GVC), creating new opportunities for participation in international trade. The global internet and data flows enable businesses to plug into GVCs to offer their own specific service. Again, in the case of Korea for example, this is a particular opportunity for new and smaller Korean firms to participate in supply chains in Asia.But restrictions on data flows are growingAs the opportunities presented by digital technologies grow, governments and regulators have to determine how to benefit from going digital while maintaining the integrity of their domestic regulations. Against this backdrop, there has been significant growth in data localization measures globally.There are various forms of restrictions on data flows. They include measures that disallow the transfer of data outside national borders; measures that allow cross-border transfers but require a copy to be maintained domestically; and requirements of prior consent before data can be transferred overseas. Data localization measures often include restrictions on data flows.Related Content Up FrontTechnology and the future of growth: Challenges of changeZia QureshiTuesday, February 25, 2020 Up FrontThe future of global manufacturingBrahima Sangafowa Coulibaly and Karim FodaWednesday, March 4, 2020 Up FrontThe future will be shaped by what global productivity growth does nextWarwick J. McKibbin and Adam TriggsMonday, March 2, 2020Measures that restrict data flows and require data to be localized are implemented for a range of reasons. One reason, which is the case with privacy regulations, is to prevent data flows to jurisdictions with lower levels of privacy protection where this undermines domestic privacy protections. For example, the European Union’s General Data Protection Regulation, which came into effect in May 2018, prohibits businesses that collect personal data in the EU from transferring it outside the EU unless the receiving country has an equivalent level of privacy protection (personal data can also be transferred in a limited number of other circumstances).Governments can also require data to be localized on the grounds that regulators need access to data in order to perform their regulatory functions. The most common of these is in the financial services sector, where data localization requirements are justified on the basis that financial regulators require financial data to remain local in case they need access to it for regulatory purposes. For instance, in 2018 India introduced a requirement that payment system operators store data locally in order to allow financial regulators to effectively perform their supervisory function.Ensuring cybersecurity is another rationale for requiring data to be localized. The view here is that data localization decreases the risks of unauthorized access. Another reason for data flows restrictions is to control access to what content can be accessed online, usually on moral, religious or political grounds. For example, Iran’s censorship aimed at creating the ”Halal Internet” limits access to content deemed offensive to Islam. China blocks access to 11 of the top 25 global sites among an estimated 3,000 prohibited foreign websites.Data localization measures are also being enacted for protectionist reasons. China’s blocking or degrading of internet access has supported the development of local champions. For instance, blocking access to Google and Facebook has been to the benefit of Baidu, Renren, and Sina Weibo.The WTO includes commitments that support digital trade and provide space for legitimate regulationTrade rules are important as enablers of digital trade and as brakes on government regulation that can restrict digital trade opportunities. The World Trade Organization (WTO) was negotiated in the late 1980s and early 1990s before the internet flourished. Yet, WTO rules are relevant for digital trade. The General Agreement on Trade in Services (GATS) is particularly relevant given the increasing scope for services trade. Where WTO members have made a commitment to allow the delivery of a service, they must also allow data to flow across borders where that is needed for the delivery of the service. As a result, data localization measures that increase the burden on foreign suppliers, such as by requiring a local presence, could be inconsistent with the GATS national treatment commitment. A WTO member could seek to justify a data localization measure under the GATS Article XIV exception provision, as being necessary to achieve an enumerated list of public policy exceptions.More comprehensive digital trade rules are being developed in free trade agreementsSince 2003, at least 70 free trade agreements (FTAs) include an e-commerce chapter. The United States-Korea FTA, for instance, includes the first explicit general commitment to cross-border data flows. However, unlike more recent FTAs, such as the Comprehensive and Progressive Agreement for Trans-Pacific Partnership (CPTPP), the commitment is only in terms of “best endeavors.” The United States-Mexico-Canada Agreement (USMCA) and the United States-Japan Trade Agreement include more comprehensive commitments, including to allow free flow of information and avoid data localization requirements, subject to appropriate exceptions provisions. FTA commitments relevant for digital trade extend beyond the digital trade chapter and include rules on intellectual property, such as a commitment to develop third-party intermediary liability regimes (as in USMCA article 20.J.11).Related Books Growth in a Time of ChangeEdited by Hyeon-Wook Kim and Zia Qureshi 2020International trade commitments at the WTO and in FTAs provide an important discipline on unnecessary restrictions on digital trade. Trade rules in FTAs are also beginning to go further than merely preventing digital trade restrictions and are also including commitments for regulatory cooperation and to develop interoperability among regulatory systems. This is significant, as more international regulatory cooperation is needed in order to give domestic regulators confidence that moving data outside of their jurisdiction won’t undermine domestic regulatory goals. At the moment, the absence of common standards or mechanisms for interoperability among regulatory systems is one reason governments are increasingly requiring data to remain local.",https://www.brookings.edu/blog/up-front/2020/03/06/data-and-the-transformation-of-international-trade/
data science,Artificial intelligence and data analytics in India,"Advances in artificial intelligence and data analytics are propelling innovation in many parts of the world.[1] China, for example, has committed $150 billion towards its goal of becoming a world leader by 2030.[2] And while the United States government is investing only $1.1 billion in non-classified AI research, its private sector is spending billions in fields from finance and healthcare to retail and defense.[3] This is transforming a number of different sectors.[4]Shamika RaviNonresident Senior Fellow - Governance Studies  Twitter@ShamikaRaviDarrell M. WestVice President and Director - Governance Studies Senior Fellow - Center for Technology Innovation  Twitter@DarrWestYet India is playing catch-up in these vital areas. It devotes only 0.6 percent of GDP to R&D, well below the 2.74 percent in the United States and 2.07 in China.[5] Its limited investment has slowed innovation and put the country at an economic disadvantage. PricewaterhouseCoopers estimates that worldwide, AI will “increase global GDP by $15.7 trillion, a full 14%, by 2030.” But of this, $7 trillion is likely to accrue to China, $3.7 trillion to North America, and only $957 billion to India.[6]In the last three years, India has attracted less than $100 million in AI-oriented venture capital financing.[7] According to writer Ananya Bhattacharya, “the sector is dominated by American firms like Accenture, Microsoft, and Adobe, which have their innovation centres here [in India]. Home-grown efforts on the academic, business, and investor fronts are few.”[8] Seventy percent of the AI research in the nation occurs in non-Indian firms. As an illustration, in looking at research publications, 62 percent of it comes from Google and IBM employees working in India and “there is only one Indian company in the top 10.”[9]There is growing AI interest, however, as India starts to invest additional resources and deploy new AI applications. This year, the national government has doubled its investment in its innovation program known as Digital India to Rs3,063 crore (or $477 million) in order to fund advances in AI, machine learning, and 3-D printing.[10] The Ministry of Commerce and Industry has developed an AI Taskforce to develop policies that encourage innovation in these areas. Its recent report emphasized the need for greater investment, more AI research, revamping of school curricula, and additional innovation by the private sector.[11]As a sign of the increased activity level, AI applications are emerging in a number of different areas that show considerable promise:FinanceFraud and corruption are major challenges for financial institutions and governmental overseers. A report by PricewaterhouseCoopers found thatRelated Technology & InnovationHow artificial intelligence is transforming the worldDarrell M. West and John R. AllenTuesday, April 24, 2018 ManufacturingGlobal manufacturing scorecard: How the US compares to 18 other nationsDarrell M. West and Christian LansangTuesday, July 10, 2018 CybersecurityWhy 5G requires new approaches to cybersecurityTom Wheeler and David SimpsonTuesday, September 3, 2019“large financial bodies such as payment regulators handle billions of transactions each day across different channels such as ATM withdrawals, credit card payments, and e-commerce transactions. Advanced analytical techniques and ML [machine learning] algorithms, combined with human expertise allow institutions to flag transactions as potentially fraudulent at the time of occurrence and hence contain the damage as early as possible.”[12]These become particularly relevant given the recent discoveries of major fraudulent transactions in several large public and private sector banks in India. AI and ML techniques can be employed to create early warning systems and minimize human errors.HealthcareAdvanced software helps health providers assess symptoms, diagnose disease, and plan appropriate treatments. According to writer Prakash Mallya, “the healthcare industry is relying on AI to fine-tune the accuracy of medical predictions and choose a fitting line of treatment.”[13] AI is a way to improve the quality of care while also containing medical costs. Beyond diagnosis and treatment, AI techniques and ML algorithms can also fine-tune interventions in public health policy across the country. There are large variations in the disease burden and demand for care across states of India, as well as across districts within large states such as Uttar Pradesh. The government can design real-time health interventions targeting specific populations, which is critically needed in Indian health policy.ManufacturingRelated Books Turning PointBy Darrell M. West and John R. Allen 2020 MarijuanaBy John Hudak 2020 Divided Politics, Divided NationBy Darrell M. West 2020AI offers the hope of improving supply chain management and resource utilization. Both of these factors are vital for manufacturing and movement up the value chain. Factories and warehouses have difficulty managing logistics and forecasting the need for particular products. Software can help track supplies and make sure companies have what they need to make their products.[14] This is particularly important given the long term strategic role of the manufacturing sector for job creation and raising overall productivity in the economy.Crime PredictionIndian authorities are using AI developed abroad to anticipate crime and intervene before it happens. For example, an Israeli company called Cortica is working with the Best Group “to analyze the terabytes of data streaming from CCTV cameras in public areas. One of the goals is to improve safety in public places, such as city streets, bus stops, and train stations.” The firm is “looking for ‘behavioral anomalies’’ that signal someone is about to commit a violent crime.”[15] Given the various security risks that India faces, application of AI and ML can be instrumental in maintaining law and order as well as neutralizing extremist threats across states of the country.Precision AgricultureFarmers are seeking to deploy AI to increase crop yields. Using sensors that measure soil temperature and moisture, these software systems identify the ideal time for planting and harvesting, and help farmers make efficient use of pest control and fertilization. Suhas Wani, Director of the International Crop Research Institute notes “sowing date as such is very critical to ensure that farmers harvest a good crop. And if it fails, it results in loss as a lot of costs are incurred for seeds, as well as the fertilizer applications.”[16] The usage of AI is also potentially significant in weather insurance, where accurate estimates of local weather conditions are instrumental in designing insurance policies for agriculture and other industries.Based on these examples, it is clear India needs to encourage greater AI investment, see more support from government agencies, make it possible for private firms to develop novel applications, reform education programs in order to generate better AI training, and encourage the venture capital community to invest in India. Adoption of these actions would help India expand its GDP and build greater economic prosperity in the future.[1] Darrell M. West, The Future of Work: Robots, AI, and Automation, Brookings Institution Press, 2018.[2] Paul Mozur, “China Sets Goal to Lead in Artificial Intelligence,” New York Times, July 21, 2017, p. B1.[3] Greg Brockman, “The Dawn of Artificial Intelligence,” Testimony before U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016.[4] Darrell M. West and John R. Allen, “How Artificial Intelligence is Transforming the World,” Brookings Institution report, April 24, 2018.[5] Darrell M. West, “How the Innovation Economy Leads to Growth,” A Hearing of the Joint Economic Committee of the U.S. Congress, April 25, 2018 and Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.[6] PricewaterhouseCoopers, “Sizing the Prize: What’s the Real Value of AI for Your Business and How Can You Capitalize?” 2017 and Rekha Menon, Madhu Vazirani, and Pradeep Roy, “Rewire for Growth: Accelerating India’s Economic Growth with Artificial Intelligence,” Accenture, 2017.[7] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.[8] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.[9] Smita Sinha, “Where Artificial Intelligence Research in India is Heading,” Analytics India, March, 2018.[10] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.[11] Artificial Intelligence Task Force, “Report of the Artificial Intelligence Task Force”, The Ministry of Commerce and Industry, Government of India, March, 2018.[12] Arnab Basu and Sudipta Ghosh, “Advance Artificial Intelligence for Growth: Leveraging AI and Robotics for India’s Economic Transformation,” PriceWaterhouseCoopers, April, 2018, p. 13.[13] Prakash Mallya, “India Wants to Go All in on AI, But Must First Tackle Shortage of Talent and Data,” Forbes, January 2, 2018.[14] Arnab Basu and Sudipta Ghosh, “Advance Artificial Intelligence for Growth: Leveraging AI and Robotics for India’s Economic Transformation,” PriceWaterhouseCoopers, April, 2018, p. 13.[15] John Quain, “Crime-Predicting A.I. Isn’t Science Fiction. It’s About to Roll Out in India,” Digital Trends, April 11, 2018.[16] Microsoft, “Digital Agriculture: Farmers in India Are Using AI to Increase Crop Yields,” undated.",https://www.brookings.edu/blog/techtank/2018/05/17/artificial-intelligence-and-data-analytics-in-india/
